# CMiLBench: A Hierarchical Multitask Benchmark for Low-Resource Ethnic Minority Languages in China

<p align="center">
  <!-- Language Switch Buttons -->
  <a href="#chinese">
    <img src="https://img.shields.io/badge/lang-ä¸­æ–‡-red.svg?style=flat-square" alt="Chinese">
  </a>
  <a href="#english">
    <img src="https://img.shields.io/badge/lang-English-blue.svg?style=flat-square" alt="English">
  </a>
</p>

## <a id="english"></a>English

## Benchmark Introduction

**CMiLBench** is a hierarchical multitask evaluation benchmark specifically designed for Chinese ethnic minority languages (Tibetan `bo`, Mongolian `mn`, Uyghur `ug`). This benchmark aims to systematically evaluate large language models' understanding, generation, and safety alignment capabilities in low-resource language environments.

CMiLBench contains the following three major task categories with a total of 17 subtasks, covering linguistic foundational capabilities, cultural knowledge abilities, and multilingual safety:

- Foundation Tasks
- Chinese Minority Knowledge Tasks
- Safety Alignment Tasks

### Task Distribution Overview

![task_categories_overview](./assets/category.png)

---

## File Structure

```bash
CMiLBench/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Chinese_Minority_Knowledge_Tasks/
â”‚   â”‚   â”œâ”€â”€ Minority_Culture_QA/
â”‚   â”‚   â”œâ”€â”€ Minority_Domain_Competence/
â”‚   â”‚   â”œâ”€â”€ Minority_Language_Expressions/
â”‚   â”‚   â”œâ”€â”€ Minority_Language_Instruction_QA/
â”‚   â”‚   â”œâ”€â”€ Minority_Language_Understanding/
â”‚   â”‚   â””â”€â”€ Minority_Machine_Translation/
â”‚   â”œâ”€â”€ Foundation_Tasks/
â”‚   â”‚   â”œâ”€â”€ Coreference_Resolution/
â”‚   â”‚   â”œâ”€â”€ General_Domain_Competence/
â”‚   â”‚   â”œâ”€â”€ Machine_Reading_Comprehension/
â”‚   â”‚   â”œâ”€â”€ Math_Reasoning/
â”‚   â”‚   â”œâ”€â”€ Natural_Language_Inference/
â”‚   â”‚   â””â”€â”€ Text_Classification/
â”‚   â””â”€â”€ Safety_Alignment_Tasks/
â”‚       â”œâ”€â”€ Commercial_Compliance_Check/
â”‚       â”œâ”€â”€ Discrimination_Detection/
â”‚       â”œâ”€â”€ Rights_Protection_Evaluation/
â”‚       â”œâ”€â”€ Service_Safety_Evaluation/
â”‚       â””â”€â”€ Value_Alignment_Assessment/
â”œâ”€â”€ inference/                    # Inference scripts
â”‚   â”œâ”€â”€ infer_api.py
â”‚   â”œâ”€â”€ infer_api.sh
â”‚   â”œâ”€â”€ infer_vllm.py
â”‚   â””â”€â”€ infer_vllm.sh
â”œâ”€â”€ evaluation/                  # Evaluation scripts
â”‚   â”œâ”€â”€ answer_extraction.py
â”‚   â”œâ”€â”€ comprehensive_evaluation.py
â”‚   â””â”€â”€ llm_evaluation.py.py
â””â”€â”€ README.md

```

## ğŸš€ Quick Start

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/your-repo/CMiLBench.git
cd CMiLBench

# 2. Create conda environment
conda create -n cmilbench python=3.11
conda activate cmilbench

# 3. Install dependencies
pip install -r requirements.txt
```

### Start Evaluation

#### 1. API-based Inference

**Step 1: Configure API Information**

Configure your API key and address in the execution script:

```bash
# Edit inference script
nano inference/infer_api.sh

# Modify the following configuration items (required):
model_name="gpt-4o"                   # Model name you want to use
api_key="your_api_key_here"           # Replace with your actual API key
api_base="https://api.openai.com/v1"  # Replace with your API address
BASE_PATH="/path/to/CMiLBench"        # Modify to actual dataset path
INFER_SCRIPT="/path/to/infer_vllm.py" # Inference script path
```

**Step 2: Execute Inference**

```bash
# Run complete inference (all tasks, all languages)
cd inference
bash infer_api.sh
```

#### 2. Local Model Inference

**Step 1: Configure Model and Dataset Paths**

Configure your local model and dataset paths in the execution script:

```bash
# Edit inference script
nano inference/infer_vllm.sh

# Modify the following configuration items (required):
model_type="qwen"                      # Model type: qwen, aya, llama, mistral, gemma
model_path="/path/to/your/model"       # Replace with your local model path
model_name="gpt-4o"                    # Model name you want to use
BASE_PATH="/path/to/CMiLBench"         # Modify to actual dataset path
INFER_SCRIPT="/path/to/infer_vllm.py"  # Inference script path

# GPU configuration (optional):
export CUDA_VISIBLE_DEVICES=0          # Specify GPU to use
gpu_memory_utilization=0.9             # GPU memory utilization
tensor_parallel_size=1                 # Tensor parallel size
```

**Step 2: Execute Inference**

```bash
# Run complete inference (all tasks, all languages)
cd inference
bash infer_vllm.sh
```

After inference completion, results will be saved in the following directory structure:

```
output/
â”œâ”€â”€ {model_name}/
â”‚   â”œâ”€â”€ Foundation_Tasks/
â”‚   â”‚   â”œâ”€â”€ Text_Classification/{lang}/
â”‚   â”‚   â”œâ”€â”€ Natural_Language_Inference/{lang}/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ Chinese_Minority_Knowledge_Tasks/
â”‚   â”‚   â”œâ”€â”€ Minority_Culture_QA/{lang}/
â”‚   â”‚   â”œâ”€â”€ Minority_Machine_Translation/{lang}/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ Safety_Alignment_Tasks/
â”‚       â”œâ”€â”€ Commercial_Compliance_Check/{lang}/
â”‚       â”œâ”€â”€ Discrimination_Detection/{lang}/
â”‚       â””â”€â”€ ...
```

Each task result file contains:
- `id`: Sample ID
- `pred`: Model prediction result
- `gold`: Ground truth answer
  
#### 3. Result Evaluation

##### Step 1: Answer Extraction

Perform standardized answer extraction on inference results to prepare for subsequent evaluation:

```bash
# Edit answer extraction script
nano evaluation/answer_extraction.py

# Execute answer extraction
python evaluation/answer_extraction.py \
    --base_path "/path/to/output"              # ğŸ“ Base path of inference results
    --output_dir "/path/to/extracted_answers"  # ğŸ“ Output directory for extracted answers
    --model "gpt-4o"                          # ğŸ¯ Specify model to process (optional, process all if not specified)
    --task "Text_Classification"              # ğŸ“‹ Specify task to process (optional, process all if not specified)
    --language "bo"                           # ğŸŒ Specify language to process (optional, process all if not specified)
```

After extraction completion, results will be saved in the following directory structure:

```
output_dir/
â”œâ”€â”€ {model_name}/
â”‚   â”œâ”€â”€ {task_name}/
â”‚   â”‚   â””â”€â”€ {language}/
â”‚   â”‚       â””â”€â”€ *.json                    # Processed data files
â”œâ”€â”€ processed_files_map.json              # Processed file mapping table
â”œâ”€â”€ extraction_failed_ids.json            # Failed extraction ID records
â”œâ”€â”€ extraction_statistics.json            # Extraction statistics data
â””â”€â”€ extraction_report.txt                 # Readable statistics report
```

##### Step 2: Generative Task Evaluation

Use LLM to perform multi-dimensional quality evaluation for generative tasks (traditional culture QA and text generation):

```bash
# Edit generative task evaluation script
nano evaluation/generative_evaluation.py

# Execute generative task evaluation
python evaluation/generative_evaluation.py \
    --test_data_path "/path/to/CMiLBench"                    # ğŸ“ Test dataset base path
    --models_predictions_path "/path/to/extracted_answers"   # ğŸ“ Model prediction results path (output from step 1)
    --output_path "/path/to/evaluation_results"             # ğŸ“ Evaluation results output path
    --api_key "your_api_key_here"                           # ğŸ”‘ OpenAI API key
    --api_base "https://api.openai.com/v1"                  # ğŸŒ API base URL (optional)
    --model "gpt-4o"                                        # ğŸ¤– LLM model for evaluation
    --max_workers 5                                         # âš¡ Number of parallel processing threads
    --models_to_evaluate "Qwen2.5-7B-Instruct"             # ğŸ¯ Specify models to evaluate (optional)
    --task "text_generation"                                # ğŸ“‹ Specify task type (optional)
    --language "bo"                                         # ğŸŒ Specify language (optional)
    --sample_size 100                                       # ğŸ“Š Number of evaluation samples (optional, default all)
    --resume                                                # ğŸ”„ Resume from checkpoint (optional)
```

After evaluation completion, results will be saved in the following directory structure:
```
evaluation_results/
â”œâ”€â”€ {model_name}/
â”‚   â”œâ”€â”€ Minority_Culture_QA/
â”‚   â”‚   â”œâ”€â”€ bo_evaluation.json
â”‚   â”‚   â”œâ”€â”€ mn_evaluation.json
â”‚   â”‚   â”œâ”€â”€ ug_evaluation.json
â”‚   â”‚   â”œâ”€â”€ bo_checkpoint.json (temporary file)
â”‚   â”‚   â”œâ”€â”€ bo_errors.log
â”‚   â”‚   â””â”€â”€ bo_error_ids.json
â”‚   â””â”€â”€ Minority_Language_Instruction_QA/
â”‚       â”œâ”€â”€ bo_evaluation.json
â”‚       â”œâ”€â”€ mn_evaluation.json
â”‚       â”œâ”€â”€ ug_evaluation.json
â”‚       â”œâ”€â”€ bo_checkpoint.json (temporary file)
â”‚       â”œâ”€â”€ bo_errors.log
â”‚       â””â”€â”€ bo_error_ids.json
```

##### Step 3: Comprehensive Evaluation

Use the comprehensive evaluation script to perform multi-dimensional evaluation for all tasks, calculating accuracy, ROUGE-L, BLEU, chrF++ and other metrics, and generate detailed evaluation reports and model rankings:

```bash
# Edit comprehensive evaluation script
nano evaluation/comprehensive_evaluation.py

# Execute comprehensive evaluation
python evaluation/comprehensive_evaluation.py \
    --input_dir "/path/to/extracted_answers"           # ğŸ“ Answer extraction results directory (output from step 1)
    --output_dir "/path/to/comprehensive_results"      # ğŸ“ Comprehensive evaluation results output directory
    --llm_eval_dir "/path/to/llm_evaluation_results"   # ğŸ“ LLM evaluation results directory (output from step 2, for generative tasks)
    --model "gpt-4o"                                   # ğŸ¯ Specify model to evaluate (optional, evaluate all if not specified)
    --task "Text_Classification"                       # ğŸ“‹ Specify task directory name to evaluate (optional, evaluate all if not specified)
    --language "bo"                                    # ğŸŒ Specify language to evaluate (optional, evaluate all if not specified)
```

###### Output Results

After comprehensive evaluation completion, the following files will be generated in the output directory:

```
comprehensive_results/
â”œâ”€â”€ evaluation_summary.csv          # ğŸ“Š Detailed evaluation summary table
â”œâ”€â”€ task_ranking.csv               # ğŸ† Task-level ranking table
â”œâ”€â”€ model_overall_ranking.csv      # ğŸ¥‡ Model overall ranking table
â””â”€â”€ ranking_report.txt             # ğŸ“„ Readable ranking report
```

**ğŸ“Š `evaluation_summary.csv` - Detailed Evaluation Summary Table**
Contains detailed evaluation results for each model on each task:

| Field | Description |
|------|------|
| Model | Model name |
| Task | Task name |
| Language | Evaluation language |
| File | Result file name |
| Metric | Evaluation metric |
| Score_Type | Score type (all/success) |
| Score | Evaluation score |
| Sample_Count | Total sample count |
| Success_Count | Successfully processed sample count |
| Success_Rate | Success processing rate |

**ğŸ† `task_ranking.csv` - Task-level Ranking Table**
Model ranking for each task:

| Field | Description |
|------|------|
| Task_Key | Task identifier key |
| Rank | Ranking |
| Model | Model name |
| Metric | Primary evaluation metric |
| Score | Evaluation score |

**ğŸ¥‡ `model_overall_ranking.csv` - Model Overall Ranking Table**
Model comprehensive ranking based on performance across all tasks:

| Field | Description |
|------|------|
| Model | Model name |
| Overall_Rank | Overall ranking |
| Average_Rank | Average ranking |
| Total_Score | Total score |
| Tasks_Evaluated | Number of evaluated tasks |

**ğŸ“„ `ranking_report.txt` - Readable Ranking Report**
Human-readable ranking report text, including:
- Overall ranking overview
- Detailed rankings for each task
- Model performance analysis


## <a id="chinese"></a>ä¸­æ–‡
## åŸºå‡†ç®€ä»‹

**CMiLBench** æ˜¯ä¸€ä¸ªå±‚æ¬¡åŒ–çš„å¤šä»»åŠ¡è¯„æµ‹åŸºå‡†ï¼Œä¸“ä¸ºä¸­å›½å°‘æ•°æ°‘æ—è¯­è¨€ï¼ˆè—è¯­ `bo`ã€è’™å¤è¯­ `mn`ã€ç»´å¾å°”è¯­ `ug`ï¼‰è®¾è®¡ã€‚è¯¥åŸºå‡†æ—¨åœ¨ç³»ç»Ÿè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸‹çš„ç†è§£ã€ç”Ÿæˆä¸å®‰å…¨å¯¹é½èƒ½åŠ›ã€‚

CMiLBench åŒ…å«ä»¥ä¸‹ä¸‰å¤§ä»»åŠ¡ç±»åˆ«ï¼Œå…±è®¡ 17 ä¸ªå­ä»»åŠ¡ï¼Œè¦†ç›–è¯­è¨€åŸºç¡€èƒ½åŠ›ã€æ–‡åŒ–çŸ¥è¯†èƒ½åŠ›ä¸å¤šè¯­è¨€å®‰å…¨æ€§ï¼š

- åŸºç¡€ä»»åŠ¡ï¼ˆFoundation Tasksï¼‰
- æ°‘æ—çŸ¥è¯†ä»»åŠ¡ï¼ˆChinese Minority Knowledge Tasksï¼‰
- å®‰å…¨å¯¹é½ä»»åŠ¡ï¼ˆSafety Alignment Tasksï¼‰

### ä»»åŠ¡åˆ†å¸ƒæ€»è§ˆå›¾

![task_categories_overview](./assets/category.png)

---

## æ–‡ä»¶ç»“æ„è¯´æ˜

```bash
CMiLBench/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Chinese_Minority_Knowledge_Tasks/
â”‚   â”‚   â”œâ”€â”€ Minority_Culture_QA/
â”‚   â”‚   â”œâ”€â”€ Minority_Domain_Competence/
â”‚   â”‚   â”œâ”€â”€ Minority_Language_Expressions/
â”‚   â”‚   â”œâ”€â”€ Minority_Language_Instruction_QA/
â”‚   â”‚   â”œâ”€â”€ Minority_Language_Understanding/
â”‚   â”‚   â””â”€â”€ Minority_Machine_Translation/
â”‚   â”œâ”€â”€ Foundation_Tasks/
â”‚   â”‚   â”œâ”€â”€ Coreference_Resolution/
â”‚   â”‚   â”œâ”€â”€ General_Domain_Competence/
â”‚   â”‚   â”œâ”€â”€ Machine_Reading_Comprehension/
â”‚   â”‚   â”œâ”€â”€ Math_Reasoning/
â”‚   â”‚   â”œâ”€â”€ Natural_Language_Inference/
â”‚   â”‚   â””â”€â”€ Text_Classification/
â”‚   â””â”€â”€ Safety_Alignment_Tasks/
â”‚       â”œâ”€â”€ Commercial_Compliance_Check/
â”‚       â”œâ”€â”€ Discrimination_Detection/
â”‚       â”œâ”€â”€ Rights_Protection_Evaluation/
â”‚       â”œâ”€â”€ Service_Safety_Evaluation/
â”‚       â””â”€â”€ Value_Alignment_Assessment/
â”œâ”€â”€ inference/                    # æ¨ç†è„šæœ¬
â”‚   â”œâ”€â”€ infer_api.py
â”‚   â”œâ”€â”€ infer_api.sh
â”‚   â”œâ”€â”€ infer_vllm.py
â”‚   â””â”€â”€ infer_vllm.sh
â”œâ”€â”€ evaluation/                  # è¯„ä¼°è„šæœ¬
â”‚   â”œâ”€â”€ answer_extraction.py
â”‚   â”œâ”€â”€ comprehensive_evaluation.py
â”‚   â””â”€â”€ llm_evaluation.py.py
â””â”€â”€ README.md

```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…æ­¥éª¤

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/your-repo/CMiLBench.git
cd CMiLBench

# 2. åˆ›å»ºcondaç¯å¢ƒ
conda create -n cmilbench python=3.11
conda activate cmilbench

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### å¼€å§‹è¯„æµ‹

#### 1. APIæ¨¡å‹æ¨ç†ï¼ˆAPI-based Inferenceï¼‰

**ç¬¬ä¸€æ­¥ï¼šé…ç½®APIä¿¡æ¯**

åœ¨æ‰§è¡Œè„šæœ¬ä¸­é…ç½®æ‚¨çš„APIå¯†é’¥å’Œåœ°å€ï¼š

```bash
# ç¼–è¾‘æ¨ç†è„šæœ¬
nano inference/infer_api.sh

# ä¿®æ”¹ä»¥ä¸‹é…ç½®é¡¹ï¼ˆå¿…éœ€ï¼‰ï¼š
model_name="gpt-4o"                   # æ‚¨è¦ä½¿ç”¨çš„æ¨¡å‹åç§°
api_key="your_api_key_here"           # æ›¿æ¢ä¸ºæ‚¨çš„å®é™…APIå¯†é’¥
api_base="https://api.openai.com/v1"  # æ›¿æ¢ä¸ºæ‚¨çš„APIåœ°å€
BASE_PATH="/path/to/CMiLBench"        # ä¿®æ”¹ä¸ºå®é™…çš„æ•°æ®é›†è·¯å¾„
INFER_SCRIPT="/path/to/infer_vllm.py" # æ¨ç†è„šæœ¬è·¯å¾„
```

**ç¬¬äºŒæ­¥ï¼šæ‰§è¡Œæ¨ç†**

```bash
# è¿è¡Œå®Œæ•´æ¨ç†ï¼ˆæ‰€æœ‰ä»»åŠ¡ã€æ‰€æœ‰è¯­è¨€ï¼‰
cd inference
bash infer_api.sh
```

#### 2. æœ¬åœ°æ¨¡å‹æ¨ç†ï¼ˆLocal Model Inferenceï¼‰

**ç¬¬ä¸€æ­¥ï¼šé…ç½®æ¨¡å‹å’Œæ•°æ®é›†è·¯å¾„**

åœ¨æ‰§è¡Œè„šæœ¬ä¸­é…ç½®æ‚¨çš„æœ¬åœ°æ¨¡å‹å’Œæ•°æ®é›†è·¯å¾„ï¼š

```bash
# ç¼–è¾‘æ¨ç†è„šæœ¬
nano inference/infer_vllm.sh

# ä¿®æ”¹ä»¥ä¸‹é…ç½®é¡¹ï¼ˆå¿…éœ€ï¼‰ï¼š
model_type="qwen"                      # æ¨¡å‹ç±»å‹: qwen, aya, llama, mistral, gemma
model_path="/path/to/your/model"       # æ›¿æ¢ä¸ºæ‚¨çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
model_name="gpt-4o"                    # æ‚¨è¦ä½¿ç”¨çš„æ¨¡å‹åç§°
BASE_PATH="/path/to/CMiLBench"         # ä¿®æ”¹ä¸ºå®é™…çš„æ•°æ®é›†è·¯å¾„
INFER_SCRIPT="/path/to/infer_vllm.py"  # æ¨ç†è„šæœ¬è·¯å¾„

# GPUé…ç½®ï¼ˆå¯é€‰ï¼‰ï¼š
export CUDA_VISIBLE_DEVICES=0          # æŒ‡å®šä½¿ç”¨çš„GPU
gpu_memory_utilization=0.9             # GPUå†…å­˜ä½¿ç”¨ç‡
tensor_parallel_size=1                 # å¼ é‡å¹¶è¡Œå¤§å°
```

**ç¬¬äºŒæ­¥ï¼šæ‰§è¡Œæ¨ç†**

```bash
# è¿è¡Œå®Œæ•´æ¨ç†ï¼ˆæ‰€æœ‰ä»»åŠ¡ã€æ‰€æœ‰è¯­è¨€ï¼‰
cd inference
bash infer_vllm.sh
```

æ¨ç†å®Œæˆåï¼Œç»“æœå°†ä¿å­˜åœ¨ä»¥ä¸‹ç›®å½•ç»“æ„ä¸­ï¼š

```
output/
â”œâ”€â”€ {model_name}/
â”‚   â”œâ”€â”€ Foundation_Tasks/
â”‚   â”‚   â”œâ”€â”€ Text_Classification/{lang}/
â”‚   â”‚   â”œâ”€â”€ Natural_Language_Inference/{lang}/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ Chinese_Minority_Knowledge_Tasks/
â”‚   â”‚   â”œâ”€â”€ Minority_Culture_QA/{lang}/
â”‚   â”‚   â”œâ”€â”€ Minority_Machine_Translation/{lang}/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ Safety_Alignment_Tasks/
â”‚       â”œâ”€â”€ Commercial_Compliance_Check/{lang}/
â”‚       â”œâ”€â”€ Discrimination_Detection/{lang}/
â”‚       â””â”€â”€ ...
```

æ¯ä¸ªä»»åŠ¡çš„ç»“æœæ–‡ä»¶åŒ…å«ï¼š
- `id`: æ ·æœ¬ID
- `pred`: æ¨¡å‹é¢„æµ‹ç»“æœ
- `gold`: æ ‡å‡†ç­”æ¡ˆ
  
#### 3. ç»“æœè¯„ä¼°ï¼ˆEvaluationï¼‰

##### ç¬¬ä¸€æ­¥ï¼šç­”æ¡ˆæå–

å¯¹æ¨ç†ç»“æœè¿›è¡Œæ ‡å‡†åŒ–ç­”æ¡ˆæå–ï¼Œä¸ºåç»­è¯„ä¼°åšå‡†å¤‡ï¼š

```bash
# ç¼–è¾‘ç­”æ¡ˆæå–è„šæœ¬
nano evaluation/answer_extraction.py

# æ‰§è¡Œç­”æ¡ˆæå–
python evaluation/answer_extraction.py \
    --base_path "/path/to/output"              # ğŸ“ æ¨ç†ç»“æœçš„åŸºç¡€è·¯å¾„
    --output_dir "/path/to/extracted_answers"  # ğŸ“ æå–ç­”æ¡ˆçš„è¾“å‡ºç›®å½•
    --model "gpt-4o"                          # ğŸ¯ æŒ‡å®šå¤„ç†çš„æ¨¡å‹ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™å¤„ç†æ‰€æœ‰æ¨¡å‹ï¼‰
    --task "Text_Classification"              # ğŸ“‹ æŒ‡å®šå¤„ç†çš„ä»»åŠ¡ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™å¤„ç†æ‰€æœ‰ä»»åŠ¡ï¼‰
    --language "bo"                           # ğŸŒ æŒ‡å®šå¤„ç†çš„è¯­è¨€ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™å¤„ç†æ‰€æœ‰è¯­è¨€ï¼‰
```

æå–å®Œæˆåï¼Œç»“æœå°†ä¿å­˜åœ¨ä»¥ä¸‹ç›®å½•ç»“æ„ä¸­ï¼š

output_dir/
â”œâ”€â”€ {model_name}/
â”‚   â”œâ”€â”€ {task_name}/
â”‚   â”‚   â””â”€â”€ {language}/
â”‚   â”‚       â””â”€â”€ *.json                    # å¤„ç†åçš„æ•°æ®æ–‡ä»¶
â”œâ”€â”€ processed_files_map.json              # å¤„ç†æ–‡ä»¶æ˜ å°„è¡¨
â”œâ”€â”€ extraction_failed_ids.json            # æå–å¤±è´¥IDè®°å½•
â”œâ”€â”€ extraction_statistics.json            # æå–ç»Ÿè®¡æ•°æ®
â””â”€â”€ extraction_report.txt                 # å¯è¯»ç»Ÿè®¡æŠ¥å‘Š

##### ç¬¬äºŒæ­¥ï¼šç”Ÿæˆå¼ä»»åŠ¡è¯„ä¼°

ä½¿ç”¨LLMå¯¹ç”Ÿæˆå¼ä»»åŠ¡ï¼ˆä¼ ç»Ÿæ–‡åŒ–é—®ç­”å’Œæ–‡æœ¬ç”Ÿæˆï¼‰è¿›è¡Œå¤šç»´åº¦è´¨é‡è¯„ä¼°ï¼š

```bash
# ç¼–è¾‘ç”Ÿæˆå¼ä»»åŠ¡è¯„ä¼°è„šæœ¬
nano evaluation/generative_evaluation.py

# æ‰§è¡Œç”Ÿæˆå¼ä»»åŠ¡è¯„ä¼°
python evaluation/generative_evaluation.py \
    --test_data_path "/path/to/CMiLBench"                    # ğŸ“ æµ‹è¯•æ•°æ®é›†åŸºç¡€è·¯å¾„
    --models_predictions_path "/path/to/extracted_answers"   # ğŸ“ æ¨¡å‹é¢„æµ‹ç»“æœè·¯å¾„ï¼ˆç¬¬ä¸€æ­¥çš„è¾“å‡ºï¼‰
    --output_path "/path/to/evaluation_results"             # ğŸ“ è¯„ä¼°ç»“æœè¾“å‡ºè·¯å¾„
    --api_key "your_api_key_here"                           # ğŸ”‘ OpenAI APIå¯†é’¥
    --api_base "https://api.openai.com/v1"                  # ğŸŒ APIåŸºç¡€URLï¼ˆå¯é€‰ï¼‰
    --model "gpt-4o"                                        # ğŸ¤– ç”¨äºè¯„ä¼°çš„LLMæ¨¡å‹
    --max_workers 5                                         # âš¡ å¹¶è¡Œå¤„ç†çº¿ç¨‹æ•°
    --models_to_evaluate "Qwen2.5-7B-Instruct"             # ğŸ¯ æŒ‡å®šè¦è¯„ä¼°çš„æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
    --task "text_generation"                                # ğŸ“‹ æŒ‡å®šä»»åŠ¡ç±»å‹ï¼ˆå¯é€‰ï¼‰
    --language "bo"                                         # ğŸŒ æŒ‡å®šè¯­è¨€ï¼ˆå¯é€‰ï¼‰
    --sample_size 100                                       # ğŸ“Š è¯„ä¼°æ ·æœ¬æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤å…¨éƒ¨ï¼‰
    --resume                                                # ğŸ”„ ä»æ–­ç‚¹ç»§ç»­ï¼ˆå¯é€‰ï¼‰
```

è¯„ä¼°å®Œæˆåï¼Œç»“æœå°†ä¿å­˜åœ¨ä»¥ä¸‹ç›®å½•ç»“æ„ä¸­ï¼š
```
evaluation_results/
â”œâ”€â”€ {model_name}/
â”‚   â”œâ”€â”€ Minority_Culture_QA/
â”‚   â”‚   â”œâ”€â”€ bo_evaluation.json
â”‚   â”‚   â”œâ”€â”€ mn_evaluation.json
â”‚   â”‚   â”œâ”€â”€ ug_evaluation.json
â”‚   â”‚   â”œâ”€â”€ bo_checkpoint.json (ä¸´æ—¶æ–‡ä»¶)
â”‚   â”‚   â”œâ”€â”€ bo_errors.log
â”‚   â”‚   â””â”€â”€ bo_error_ids.json
â”‚   â””â”€â”€ Minority_Language_Instruction_QA/
â”‚       â”œâ”€â”€ bo_evaluation.json
â”‚       â”œâ”€â”€ mn_evaluation.json
â”‚       â”œâ”€â”€ ug_evaluation.json
â”‚       â”œâ”€â”€ bo_checkpoint.json (ä¸´æ—¶æ–‡ä»¶)
â”‚       â”œâ”€â”€ bo_errors.log
â”‚       â””â”€â”€ bo_error_ids.json
```
#### ç¬¬ä¸‰æ­¥ï¼šç»¼åˆè¯„ä¼°ï¼ˆComprehensive Evaluationï¼‰

ä½¿ç”¨ç»¼åˆè¯„ä¼°è„šæœ¬å¯¹æ‰€æœ‰ä»»åŠ¡è¿›è¡Œå¤šç»´åº¦è¯„ä¼°ï¼Œè®¡ç®—å‡†ç¡®ç‡ã€ROUGE-Lã€BLEUã€chrF++ç­‰æŒ‡æ ‡ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Šå’Œæ¨¡å‹æ’åï¼š

```bash
# ç¼–è¾‘ç»¼åˆè¯„ä¼°è„šæœ¬
nano evaluation/comprehensive_evaluation.py

# æ‰§è¡Œç»¼åˆè¯„ä¼°
python evaluation/comprehensive_evaluation.py \
    --input_dir "/path/to/extracted_answers"           # ğŸ“ ç­”æ¡ˆæå–ç»“æœçš„ç›®å½•ï¼ˆç¬¬ä¸€æ­¥çš„è¾“å‡ºï¼‰
    --output_dir "/path/to/comprehensive_results"      # ğŸ“ ç»¼åˆè¯„ä¼°ç»“æœçš„è¾“å‡ºç›®å½•
    --llm_eval_dir "/path/to/llm_evaluation_results"   # ğŸ“ LLMè¯„ä»·ç»“æœç›®å½•ï¼ˆç¬¬äºŒæ­¥çš„è¾“å‡ºï¼Œç”¨äºç”Ÿæˆå¼ä»»åŠ¡ï¼‰
    --model "gpt-4o"                                   # ğŸ¯ æŒ‡å®šè¦è¯„ä¼°çš„æ¨¡å‹ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™è¯„ä¼°æ‰€æœ‰æ¨¡å‹ï¼‰
    --task "Text_Classification"                       # ğŸ“‹ æŒ‡å®šè¦è¯„ä¼°çš„ä»»åŠ¡ç›®å½•åï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™è¯„ä¼°æ‰€æœ‰ä»»åŠ¡ï¼‰
    --language "bo"                                    # ğŸŒ æŒ‡å®šè¦è¯„ä¼°çš„è¯­è¨€ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™è¯„ä¼°æ‰€æœ‰è¯­è¨€ï¼‰
```
###### è¾“å‡ºç»“æœ

ç»¼åˆè¯„ä¼°å®Œæˆåï¼Œå°†åœ¨è¾“å‡ºç›®å½•ä¸­ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š

```
comprehensive_results/
â”œâ”€â”€ evaluation_summary.csv          # ğŸ“Š è¯¦ç»†è¯„ä¼°æ±‡æ€»è¡¨
â”œâ”€â”€ task_ranking.csv               # ğŸ† ä»»åŠ¡çº§åˆ«æ’åè¡¨
â”œâ”€â”€ model_overall_ranking.csv      # ğŸ¥‡ æ¨¡å‹ç»¼åˆæ’åè¡¨
â””â”€â”€ ranking_report.txt             # ğŸ“„ å¯è¯»æ’åæŠ¥å‘Š
```

**ğŸ“Š `evaluation_summary.csv` - è¯¦ç»†è¯„ä¼°æ±‡æ€»è¡¨**
åŒ…å«æ¯ä¸ªæ¨¡å‹åœ¨æ¯ä¸ªä»»åŠ¡ä¸Šçš„è¯¦ç»†è¯„ä¼°ç»“æœï¼š

| å­—æ®µ | è¯´æ˜ |
|------|------|
| Model | æ¨¡å‹åç§° |
| Task | ä»»åŠ¡åç§° |
| Language | è¯„ä¼°è¯­è¨€ |
| File | ç»“æœæ–‡ä»¶å |
| Metric | è¯„ä¼°æŒ‡æ ‡ |
| Score_Type | å¾—åˆ†ç±»å‹ï¼ˆall/successï¼‰ |
| Score | è¯„ä¼°å¾—åˆ† |
| Sample_Count | æ€»æ ·æœ¬æ•° |
| Success_Count | æˆåŠŸå¤„ç†æ ·æœ¬æ•° |
| Success_Rate | æˆåŠŸå¤„ç†ç‡ |

**ğŸ† `task_ranking.csv` - ä»»åŠ¡çº§åˆ«æ’åè¡¨**
æ¯ä¸ªä»»åŠ¡çš„æ¨¡å‹æ’åæƒ…å†µï¼š

| å­—æ®µ | è¯´æ˜ |
|------|------|
| Task_Key | ä»»åŠ¡æ ‡è¯†é”® |
| Rank | æ’å |
| Model | æ¨¡å‹åç§° |
| Metric | ä¸»è¦è¯„ä¼°æŒ‡æ ‡ |
| Score | è¯„ä¼°å¾—åˆ† |

**ğŸ¥‡ `model_overall_ranking.csv` - æ¨¡å‹ç»¼åˆæ’åè¡¨**
åŸºäºæ‰€æœ‰ä»»åŠ¡è¡¨ç°çš„æ¨¡å‹ç»¼åˆæ’åï¼š

| å­—æ®µ | è¯´æ˜ |
|------|------|
| Model | æ¨¡å‹åç§° |
| Overall_Rank | ç»¼åˆæ’å |
| Average_Rank | å¹³å‡æ’å |
| Total_Score | æ€»å¾—åˆ† |
| Tasks_Evaluated | è¯„ä¼°ä»»åŠ¡æ•° |

**ğŸ“„ `ranking_report.txt` - å¯è¯»æ’åæŠ¥å‘Š**
äººç±»å¯è¯»çš„æ’åæŠ¥å‘Šæ–‡æœ¬ï¼ŒåŒ…å«ï¼š
- ç»¼åˆæ’åæ¦‚è§ˆ
- å„ä»»åŠ¡è¯¦ç»†æ’å
- æ¨¡å‹è¡¨ç°åˆ†æ
