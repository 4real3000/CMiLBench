import os
import json
import time
import re
import argparse
from tqdm import tqdm
from openai import OpenAI
import concurrent.futures
import sys
import signal
import traceback
from datetime import datetime
import random

LANGUAGE_NAMES = {"zh": "æ±‰è¯­", "bo": "è—è¯­", "mn": "è’™å¤è¯­", "ug": "ç»´å¾å°”è¯­"}

# åªå¤„ç†ä¸¤ä¸ªç”Ÿæˆå¼ä»»åŠ¡ï¼ˆä¸åŒ…å«ç¿»è¯‘ï¼‰
TASK_TYPES = {
    "traditional_culture": "ä¼ ç»Ÿæ–‡åŒ–", 
    "text_generation": "æ–‡æœ¬ç”Ÿæˆ"
}

# æ–°æ—§ä»»åŠ¡åç§°æ˜ å°„ (åªåŒ…å«éœ€è¦çš„ä¸¤ä¸ªä»»åŠ¡)
TASK_MAPPING = {
    'Minority_Culture_QA': 'traditional_culture',
    'Minority_Language_Instruction_QA': 'text_generation',
}

# æ·»åŠ åå‘æ˜ å°„ï¼šä»æ—§åç§°åˆ°æ–°åç§°
REVERSE_TASK_MAPPING = {v: k for k, v in TASK_MAPPING.items()}

GRACEFUL_EXIT_REQUESTED = False

# --- Signal Handler ---
def signal_handler(sig, frame):
    global GRACEFUL_EXIT_REQUESTED
    if not GRACEFUL_EXIT_REQUESTED:
        print(f"\næ•è·åˆ°ä¿¡å· {sig}, è¯·æ±‚ä¼˜é›…é€€å‡º... åç»­æ–°çš„APIè°ƒç”¨å°†ä¸­æ­¢ï¼Œå½“å‰æ‰¹æ¬¡å®Œæˆåå°†ä¿å­˜ã€‚")
        print("å¦‚æœç¨‹åºæ— æ³•æ­£å¸¸é€€å‡ºï¼Œè¯·å†æ¬¡æŒ‰ Ctrl+C å¼ºåˆ¶é€€å‡ºã€‚")
        GRACEFUL_EXIT_REQUESTED = True
    else:
        print(f"\nå†æ¬¡æ•è·åˆ°ä¿¡å· {sig}, å¼ºåˆ¶é€€å‡ºç¨‹åº...")
        print("æ­£åœ¨å°è¯•ä¿å­˜å½“å‰è¿›åº¦...")
        sys.exit(1)

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# --- Utility Functions ---
def get_client(api_key, api_base=None):
    return OpenAI(api_key=api_key, base_url=api_base) if api_base else OpenAI(api_key=api_key)

def save_json_safely(data, filepath):
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    temp_filepath = filepath + ".tmp"
    try:
        with open(temp_filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        if os.path.exists(filepath):
            os.remove(filepath)
        os.rename(temp_filepath, filepath)
        return True
    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶ {filepath} æ—¶å‡ºé”™: {str(e)}")
        if os.path.exists(temp_filepath):
            try: os.remove(temp_filepath)
            except OSError: pass
        return False

def log_error(error_log_file, error_id_file, message, error=None, item_ids=None, details=""):
    """è®°å½•é”™è¯¯åˆ°æ—¥å¿—æ–‡ä»¶å¹¶ä¿å­˜ç›¸å…³ID"""
    os.makedirs(os.path.dirname(error_log_file), exist_ok=True)
    os.makedirs(os.path.dirname(error_id_file), exist_ok=True)
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    log_entry = f"=== {timestamp} ===\n"
    log_entry += f"é”™è¯¯ä¿¡æ¯: {message}\n"
    if error:
        log_entry += f"å¼‚å¸¸ç±»å‹: {type(error).__name__}\n"
        log_entry += f"å¼‚å¸¸è¯¦æƒ…: {str(error)}\n"
        log_entry += f"å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}\n"
    if details:
        log_entry += f"è¡¥å……ä¿¡æ¯: {details}\n"
    log_entry += "\n"
    
    try:
        with open(error_log_file, 'a', encoding='utf-8') as f:
            f.write(log_entry)
    except Exception as e_log:
        print(f"å†™å…¥é”™è¯¯æ—¥å¿— {error_log_file} å¤±è´¥: {e_log}")
    
    # ä¿å­˜å‡ºé”™çš„ID
    if item_ids:
        try:
            existing_ids = []
            if os.path.exists(error_id_file):
                try:
                    with open(error_id_file, 'r', encoding='utf-8') as f:
                        existing_ids = json.load(f)
                    if not isinstance(existing_ids, list):
                        existing_ids = []
                except (json.JSONDecodeError, FileNotFoundError):
                    existing_ids = []
            
            for item_id in item_ids:
                if item_id and item_id not in existing_ids:
                    existing_ids.append(item_id)
            
            save_json_safely(existing_ids, error_id_file)
            print(f"å·²å°†{len(item_ids)}ä¸ªé”™è¯¯IDä¿å­˜åˆ° {error_id_file}")
        except Exception as e:
            print(f"ä¿å­˜é”™è¯¯IDæ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")

# --- Core Logic Functions ---
def load_test_data(task_type, language, base_path):
    """æ ¹æ®æ–°çš„ç›®å½•ç»“æ„åŠ è½½æµ‹è¯•æ•°æ®"""
    
    # æ ¹æ®ä»»åŠ¡ç±»å‹ç¡®å®šæ•°æ®æ–‡ä»¶è·¯å¾„
    if task_type == "traditional_culture":
        file_path = os.path.join(base_path, f"Chinese_Minority_Knowledge_Tasks/Minority_Culture_QA/{language}.json")
    elif task_type == "text_generation":
        file_path = os.path.join(base_path, f"Chinese_Minority_Knowledge_Tasks/Minority_Language_Instruction_QA/{language}.json")
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {task_type}")
    
    if not os.path.exists(file_path):
        print(f"è­¦å‘Š: æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return {}
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f: 
            data = json.load(f)
        id_to_data = {item.get('id', item.get('query_id')): item for item in data if item.get('id') or item.get('query_id')}
        return id_to_data
    except Exception as e:
        print(f"åŠ è½½æµ‹è¯•æ•°æ® {file_path} å‡ºé”™: {str(e)}")
        return {}

def load_model_predictions(model_name, task_type, language_param, models_base_path):
    """åŠ è½½æ¨¡å‹é¢„æµ‹ç»“æœ - ä¿®æ”¹ä¸ºä½¿ç”¨æ–°çš„æ–‡ä»¶å¤¹ç»“æ„"""
    try:
        if not language_param:
            raise ValueError(f"{task_type} ä»»åŠ¡éœ€è¦æŒ‡å®šè¯­è¨€å‚æ•°ã€‚")
        
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šå°†æ—§ä»»åŠ¡åç§°è½¬æ¢ä¸ºæ–°çš„æ–‡ä»¶å¤¹åç§°
        new_task_name = REVERSE_TASK_MAPPING.get(task_type, task_type)
        file_path = os.path.join(models_base_path, model_name, new_task_name, language_param, "zh-prompt_test.json")
        
        if not os.path.exists(file_path):
            print(f"æ¨¡å‹é¢„æµ‹æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None
        
        with open(file_path, 'r', encoding='utf-8') as f: 
            predictions = json.load(f)
        return predictions
    except Exception as e:
        print(f"åŠ è½½æ¨¡å‹ {model_name} ({task_type}, {language_param}) çš„é¢„æµ‹ç»“æœå‡ºé”™: {str(e)}")
        return None

def construct_prompt(task_type, language, question, reference, prediction, subcategory=None):
    """æ„å»ºè¯„ä¼°æç¤º"""
    # é€šç”¨è¯„åˆ†æ ‡å‡†
    scoring_criteria = """
è¯„åˆ†æ ‡å‡†ï¼ˆ1-5åˆ†ï¼‰ï¼š
- 5åˆ†ï¼šä¼˜ç§€ - å®Œå…¨ç¬¦åˆè¦æ±‚ï¼Œè¡¨ç°å“è¶Š
- 4åˆ†ï¼šè‰¯å¥½ - åŸºæœ¬ç¬¦åˆè¦æ±‚ï¼Œè¡¨ç°è¾ƒå¥½ï¼Œæœ‰è½»å¾®ä¸è¶³
- 3åˆ†ï¼šä¸€èˆ¬ - éƒ¨åˆ†ç¬¦åˆè¦æ±‚ï¼Œè¡¨ç°ä¸­ç­‰ï¼Œæœ‰æ˜æ˜¾ä¸è¶³
- 2åˆ†ï¼šè¾ƒå·® - å°‘éƒ¨åˆ†ç¬¦åˆè¦æ±‚ï¼Œè¡¨ç°ä¸ä½³ï¼Œé—®é¢˜è¾ƒå¤š
- 1åˆ†ï¼šå¾ˆå·® - ä¸ç¬¦åˆè¦æ±‚ï¼Œè¡¨ç°å¾ˆå·®ï¼Œé—®é¢˜ä¸¥é‡
"""

    # è¯„ä¼°ç»´åº¦å®šä¹‰
    dimension_definitions = {
        "äº‹å®æ­£ç¡®æ€§": "å›ç­”ä¸­æä¾›çš„ä¿¡æ¯æ˜¯å¦å‡†ç¡®æ— è¯¯ï¼Œæ˜¯å¦åŸºäºå¯ä¿¡çš„äº‹å®å’Œæ•°æ®",
        "æ»¡è¶³ç”¨æˆ·éœ€æ±‚": "å›ç­”æ˜¯å¦æ»¡è¶³äº†ç”¨æˆ·æå‡ºé—®é¢˜çš„ç›®çš„å’Œéœ€æ±‚ï¼Œæ˜¯å¦å¯¹é—®é¢˜è¿›è¡Œäº†å…¨é¢è€Œæ°å½“çš„å›åº”",
        "å®‰å…¨æ— å®³": "å›ç­”æ˜¯å¦ä¸ä¼šå¼•å‘å±é™©æˆ–æœ‰å®³è¡Œä¸ºï¼Œæ˜¯å¦ä¸ä¼šè¯¯å¯¼ç”¨æˆ·æˆ–ä¼ æ’­æœ‰å®³ä¿¡æ¯",
        "æ¸…æ™°åº¦": "å›ç­”æ˜¯å¦è¡¨è¾¾æ¸…æ™°ã€æ˜“æ‡‚ï¼Œæ˜¯å¦ä½¿ç”¨äº†ç®€æ´çš„è¯­è¨€å’Œç»“æ„ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥è½»æ¾ç†è§£",
        "é€»è¾‘æ€§": "å›ç­”æ˜¯å¦åœ¨é€»è¾‘æˆ–è€…æ¨ç†ä¸Šè¿è´¯ä¸”åˆç†",
        "å®Œå¤‡æ€§": "å›ç­”æ˜¯å¦æä¾›äº†è¶³å¤Ÿçš„ä¿¡æ¯å’Œç»†èŠ‚ï¼Œä»¥æ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚ï¼Œæ˜¯å¦é—æ¼äº†é‡è¦çš„æ–¹é¢",
        "åˆ›é€ æ€§": "å›ç­”æ˜¯å¦å…·æœ‰åˆ›æ–°æ€§æˆ–ç‹¬ç‰¹æ€§ï¼Œæ˜¯å¦æä¾›äº†æ–°é¢–çš„è§è§£æˆ–è§£å†³æ–¹æ³•",
        "å¯è´Ÿè´£ç¨‹åº¦": "å›ç­”ä¸­æä¾›çš„å»ºè®®æˆ–ä¿¡æ¯æ˜¯å¦å¯è¡Œï¼Œæ˜¯å¦è´Ÿæœ‰ä¸€å®šçš„è´£ä»»ï¼Œæ˜¯å¦è€ƒè™‘äº†æ½œåœ¨é£é™©å’Œåæœ",
        "é€»è¾‘è¿è´¯æ€§": "å›ç­”æ˜¯å¦åœ¨æ•´ä½“ä¸Šä¿æŒä¸€è‡´ï¼Œæ˜¯å¦åœ¨ä¸åŒéƒ¨åˆ†ä¹‹é—´ä¿æŒé€»è¾‘è¿è´¯æ€§ï¼Œé¿å…äº†è‡ªç›¸çŸ›ç›¾",
        "å…¬å¹³ä¸å¯è´Ÿè´£ç¨‹åº¦": "å›ç­”æ˜¯å¦è€ƒè™‘äº†ä¸åŒè§‚ç‚¹å’Œç«‹åœºï¼Œæ˜¯å¦æä¾›äº†å…¬æ­£çš„ä¿¡æ¯æˆ–å»ºè®®ï¼Œä¸æºå¸¦ç§äººçš„è§‚ç‚¹æˆ–è€…åè§ï¼Œä¸åè¢’æŸä¸€æ–¹ï¼Œæä¾›çš„å»ºè®®æˆ–ä¿¡æ¯æ˜¯å¦å¯è¡Œï¼Œæ˜¯å¦è´Ÿæœ‰ä¸€å®šçš„è´£ä»»ï¼Œæ˜¯å¦è€ƒè™‘äº†æ½œåœ¨é£é™©å’Œåæœ",
        "ä¸°å¯Œåº¦": "å›ç­”åŒ…å«ä¸°å¯Œçš„ä¿¡æ¯ã€æ·±åº¦ã€ä¸Šä¸‹æ–‡è€ƒè™‘ã€å¤šæ ·æ€§ã€è¯¦ç»†è§£é‡Šå’Œå®ä¾‹ï¼Œä»¥æ»¡è¶³ç”¨æˆ·éœ€æ±‚å¹¶æä¾›å…¨é¢ç†è§£"
    }

    # å›ç­”ç±»å‹åˆ°è¯„ä¼°ç»´åº¦çš„æ˜ å°„
    answer_type_dimensions = {
        "äº‹å®ä¸è§£é‡Šå‹å›ç­”": ["äº‹å®æ­£ç¡®æ€§", "æ»¡è¶³ç”¨æˆ·éœ€æ±‚", "æ¸…æ™°åº¦", "å®Œå¤‡æ€§"],
        "é€»è¾‘æ¨ç†å‹å›ç­”": ["äº‹å®æ­£ç¡®æ€§", "æ»¡è¶³ç”¨æˆ·éœ€æ±‚", "é€»è¾‘è¿è´¯æ€§", "å®Œå¤‡æ€§"],
        "ç”Ÿæˆå‹å›ç­”": ["äº‹å®æ­£ç¡®æ€§", "æ»¡è¶³ç”¨æˆ·éœ€æ±‚", "é€»è¾‘è¿è´¯æ€§", "åˆ›é€ æ€§", "ä¸°å¯Œåº¦"],
        "å»ºè®®å‹å›ç­”": ["äº‹å®æ­£ç¡®æ€§", "æ»¡è¶³ç”¨æˆ·éœ€æ±‚", "å…¬å¹³ä¸å¯è´Ÿè´£ç¨‹åº¦", "åˆ›é€ æ€§"]
    }

    # å­ç±»åˆ«åˆ°å›ç­”ç±»å‹çš„æ˜ å°„
    subcategory_to_answer_type = {
        "å¸¸è¯†çŸ¥è¯†": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "é˜…è¯»ç†è§£": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "ç¿»è¯‘": "ç”Ÿæˆå‹å›ç­”",
        "æ–‡æœ¬åˆ†ç±»": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "ä¿¡æ¯æŠ½å–": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "å­—è¯ç†è§£": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "æ–‡åŒ–ç†è§£": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "è§‚ç‚¹è¡¨è¾¾": "å»ºè®®å‹å›ç­”",
        "å¯»æ±‚å»ºè®®": "å»ºè®®å‹å›ç­”",
        "å®ç”¨æ–‡ä½“å†™ä½œ": "ç”Ÿæˆå‹å›ç­”",
        "åˆ›æ„æ–‡ä½“å†™ä½œ": "ç”Ÿæˆå‹å›ç­”",
        "ä¸“ä¸šæ–‡ä½“å†™ä½œ": "ç”Ÿæˆå‹å›ç­”",
        "å…¶ä»–å†™ä½œç±»": "ç”Ÿæˆå‹å›ç­”",
        "è¯æ˜": "é€»è¾‘æ¨ç†å‹å›ç­”",
        "æ¨ç†": "é€»è¾‘æ¨ç†å‹å›ç­”",
        "åˆç­‰æ•°å­¦": "é€»è¾‘æ¨ç†å‹å›ç­”",
        "é«˜ç­‰æ•°å­¦": "é€»è¾‘æ¨ç†å‹å›ç­”",
        "åº”ç”¨æ•°å­¦": "é€»è¾‘æ¨ç†å‹å›ç­”",
        "ç°å®ç”Ÿæ´»ç±»": "ç”Ÿæˆå‹å›ç­”",
        "æ¸¸æˆå¨±ä¹ç±»": "ç”Ÿæˆå‹å›ç­”",
        "åŠŸèƒ½ç±»": "ç”Ÿæˆå‹å›ç­”",
        "ç°å®åäººç±»": "ç”Ÿæˆå‹å›ç­”",
        "ï¼ˆè™šæ‹Ÿï¼‰æ‹çˆ±ç±»": "ç”Ÿæˆå‹å›ç­”",
        "ç‰©ç†": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "åŒ–å­¦": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "è®¡ç®—æœº": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "ç”Ÿç‰©åŒ»å­¦": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "ç»æµ": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "å¤©æ–‡": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "å†å²": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "éŸ³ä¹": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "æ³•å¾‹": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "ä½“è‚²": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "åœ°ç†": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "æ–‡å­¦": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "å…¶ä»–": "äº‹å®ä¸è§£é‡Šå‹å›ç­”"
    }

    if task_type == "text_generation":
        # æ ¹æ®subcategoryç¡®å®šè¯„ä¼°ç»´åº¦
        if subcategory and subcategory in subcategory_to_answer_type:
            answer_type = subcategory_to_answer_type[subcategory]
            dimensions = answer_type_dimensions[answer_type].copy()
        else:
            # é»˜è®¤ä½¿ç”¨ç”Ÿæˆå‹å›ç­”çš„ç»´åº¦
            answer_type = "ç”Ÿæˆå‹å›ç­”"
            dimensions = answer_type_dimensions[answer_type].copy()
        
        # æ·»åŠ è¯­è¨€ä½¿ç”¨å‡†ç¡®æ€§ç»´åº¦
        dimensions.append("è¯­è¨€ä½¿ç”¨å‡†ç¡®æ€§")
        
        # æ„å»ºè¯„ä¼°ç»´åº¦è¯´æ˜
        dimension_descriptions = []
        for i, dim in enumerate(dimensions, 1):
            if dim == "è¯­è¨€ä½¿ç”¨å‡†ç¡®æ€§":
                dimension_descriptions.append(f"{i}. {dim}ï¼šè¯„ä¼°å›ç­”æ˜¯å¦ä½¿ç”¨äº†æ­£ç¡®çš„ç›®æ ‡è¯­è¨€ï¼ˆ{LANGUAGE_NAMES.get(language, language)}ï¼‰ï¼Œæ˜¯å¦å­˜åœ¨è¯­è¨€æ··ç”¨æˆ–ä½¿ç”¨é”™è¯¯è¯­è¨€çš„æƒ…å†µ")
            else:
                dimension_descriptions.append(f"{i}. {dim}ï¼š{dimension_definitions[dim]}")
        
        # æ„å»ºè¾“å‡ºæ ¼å¼
        output_format = []
        for dim in dimensions:
            output_format.append(f"{dim}ï¼š[1-5çš„æ•°å­—]")
        
        prompt = f"""è¯·ä½ ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„{LANGUAGE_NAMES.get(language, language)}æ–‡æœ¬ç”Ÿæˆè´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œå¯¹ä»¥ä¸‹æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬è¿›è¡Œå…¨é¢è¯„åˆ†ã€‚

ä»»åŠ¡è¯­è¨€ï¼š{LANGUAGE_NAMES.get(language, language)}
å­ç±»åˆ«ï¼š{subcategory if subcategory else 'æœªæŒ‡å®š'}
è¯„ä¼°ç±»å‹ï¼š{answer_type}

é—®é¢˜/æç¤ºï¼š
{question}

å‚è€ƒç­”æ¡ˆï¼š
{reference}

æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ï¼š
{prediction}

{scoring_criteria}

è¯·ä»ä»¥ä¸‹{len(dimensions)}ä¸ªç»´åº¦å¯¹ç”Ÿæˆæ–‡æœ¬è´¨é‡è¿›è¡Œè¯„åˆ†ï¼ˆæ¯ä¸ªç»´åº¦1-5åˆ†ï¼‰ï¼š

{chr(10).join(dimension_descriptions)}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºè¯„åˆ†ç»“æœï¼š
{chr(10).join(output_format)}
åˆ†ææ€»ç»“ï¼š[ç®€è¦åˆ†æç”Ÿæˆæ–‡æœ¬çš„ä¼˜ç¼ºç‚¹å’Œè´¨é‡ç‰¹ç‚¹ï¼Œ100-200å­—]
æœ€ç»ˆåˆ†æ•°ï¼š[{len(dimensions)}ä¸ªç»´åº¦çš„å¹³å‡åˆ†ï¼Œä¿ç•™ä¸¤ä½å°æ•°]"""

    elif task_type == "traditional_culture":
        prompt = f"""è¯·ä½ ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„{LANGUAGE_NAMES.get(language, language)}ä¼ ç»Ÿæ–‡åŒ–ä¸“å®¶ï¼Œå¯¹ä»¥ä¸‹æ¨¡å‹å›ç­”è¿›è¡Œå…¨é¢è¯„åˆ†ã€‚

ä»»åŠ¡è¯­è¨€ï¼š{LANGUAGE_NAMES.get(language, language)}

é—®é¢˜ï¼š
{question}

å‚è€ƒç­”æ¡ˆï¼š
{reference}

æ¨¡å‹å›ç­”ï¼š
{prediction}

{scoring_criteria}

è¯·ä»ä»¥ä¸‹6ä¸ªç»´åº¦å¯¹å›ç­”è´¨é‡è¿›è¡Œè¯„åˆ†ï¼ˆæ¯ä¸ªç»´åº¦1-5åˆ†ï¼‰ï¼š

1. çŸ¥è¯†å‡†ç¡®æ€§ï¼ˆæ–‡åŒ–çŸ¥è¯†æ˜¯å¦å‡†ç¡®ï¼‰ï¼šè¯„ä¼°å›ç­”ä¸­æ–‡åŒ–çŸ¥è¯†ç‚¹çš„å‡†ç¡®æ€§å’Œå¯é æ€§
2. æ–‡åŒ–ç†è§£æ·±åº¦ï¼ˆå¯¹æ–‡åŒ–å†…æ¶µçš„ç†è§£ç¨‹åº¦ï¼‰ï¼šè¯„ä¼°å¯¹æ–‡åŒ–èƒŒæ™¯ã€å†…æ¶µå’Œæ„ä¹‰çš„æ·±å…¥ç†è§£
3. è¯­è¨€è¡¨è¾¾é€‚åˆ‡æ€§ï¼ˆè¯­è¨€ä½¿ç”¨æ˜¯å¦å¾—ä½“ï¼‰ï¼šè¯„ä¼°è¯­è¨€è¡¨è¾¾æ˜¯å¦ç¬¦åˆæ–‡åŒ–è¯­å¢ƒå’Œè¡¨è¾¾ä¹ æƒ¯
4. å†…å®¹å®Œæ•´æ€§ï¼ˆå›ç­”æ˜¯å¦å…¨é¢å®Œæ•´ï¼‰ï¼šè¯„ä¼°å›ç­”çš„å®Œæ•´æ€§å’Œå…¨é¢æ€§
5. å†…éƒ¨è§†è§’çœŸå®æ€§ï¼ˆæ˜¯å¦ä½“ç°è¯¥æ–‡åŒ–çš„å†…éƒ¨è§†è§’ï¼‰ï¼šè¯„ä¼°æ˜¯å¦çœŸå®åæ˜ äº†è¯¥æ–‡åŒ–ç¾¤ä½“çš„å†…éƒ¨è§‚ç‚¹å’Œè®¤çŸ¥
6. è¯­è¨€ä½¿ç”¨å‡†ç¡®æ€§ï¼ˆæ˜¯å¦ä½¿ç”¨æ­£ç¡®çš„ç›®æ ‡è¯­è¨€ï¼‰ï¼šè¯„ä¼°å›ç­”æ˜¯å¦ä½¿ç”¨äº†æ­£ç¡®çš„ç›®æ ‡è¯­è¨€ï¼ˆ{LANGUAGE_NAMES.get(language, language)}ï¼‰ï¼Œæ˜¯å¦å­˜åœ¨è¯­è¨€æ··ç”¨æˆ–ä½¿ç”¨é”™è¯¯è¯­è¨€çš„æƒ…å†µ

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºè¯„åˆ†ç»“æœï¼š
çŸ¥è¯†å‡†ç¡®æ€§ï¼š[1-5çš„æ•°å­—]
æ–‡åŒ–ç†è§£æ·±åº¦ï¼š[1-5çš„æ•°å­—]
è¯­è¨€è¡¨è¾¾é€‚åˆ‡æ€§ï¼š[1-5çš„æ•°å­—]
å†…å®¹å®Œæ•´æ€§ï¼š[1-5çš„æ•°å­—]
å†…éƒ¨è§†è§’çœŸå®æ€§ï¼š[1-5çš„æ•°å­—]
è¯­è¨€ä½¿ç”¨å‡†ç¡®æ€§ï¼š[1-5çš„æ•°å­—]
åˆ†ææ€»ç»“ï¼š[ç®€è¦åˆ†æå›ç­”åœ¨æ–‡åŒ–ç†è§£å’ŒçŸ¥è¯†è¡¨è¾¾æ–¹é¢çš„ä¼˜ç¼ºç‚¹ï¼Œ100-200å­—]
æœ€ç»ˆåˆ†æ•°ï¼š[å…­ä¸ªç»´åº¦çš„å¹³å‡åˆ†ï¼Œä¿ç•™ä¸¤ä½å°æ•°]"""
    
    else:
        print(f"è­¦å‘Š: æœªçŸ¥ä»»åŠ¡ç±»å‹ '{task_type}' æ— æ³•æ„å»ºæç¤ºã€‚")
        return "é”™è¯¯ï¼šæœªçŸ¥ä»»åŠ¡ç±»å‹ã€‚"
    
    return prompt

def evaluate_sample(client, model, task_type, language, question, reference, prediction, subcategory=None):
    global GRACEFUL_EXIT_REQUESTED
    
    if GRACEFUL_EXIT_REQUESTED:
        return "è¯„ä¼°APIè°ƒç”¨å› è„šæœ¬é€€å‡ºè€Œä¸­æ­¢"
    
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    if task_type == "text_generation" and subcategory:
        print(f"Debug: æ­£åœ¨æ„å»º{task_type}ä»»åŠ¡çš„promptï¼Œsubcategory: {subcategory}")
    
    prompt = construct_prompt(task_type, language, question, reference, prediction, subcategory)
    
    # æ£€æŸ¥promptæ˜¯å¦æ„å»ºæˆåŠŸ
    if prompt.startswith("é”™è¯¯ï¼š"):
        return f"æ„å»ºpromptå¤±è´¥: {prompt}"
    
    max_retries = 3
    retry_delay = 2
    
    for attempt in range(max_retries):
        if GRACEFUL_EXIT_REQUESTED:
            return "è¯„ä¼°APIè°ƒç”¨å› è„šæœ¬é€€å‡ºè€Œä¸­æ­¢"
        
        try:
            # æ·»åŠ è¶…æ—¶æœºåˆ¶
            response = client.chat.completions.create(
                model=model, 
                messages=[{"role": "user", "content": prompt}], 
                temperature=0.2,
                timeout=30  # 30ç§’è¶…æ—¶
            )
            result = response.choices[0].message.content.strip()
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼Œæ£€æŸ¥APIå“åº”
            if task_type == "text_generation" and subcategory:
                print(f"Debug: APIå“åº”æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(result)}")
            
            return result
        except Exception as e:
            if GRACEFUL_EXIT_REQUESTED:
                return f"è¯„ä¼°APIè°ƒç”¨å› è„šæœ¬é€€å‡ºè€Œä¸­æ­¢: {str(e)}"
            
            error_msg = f"{type(e).__name__}: {str(e)}"
            if attempt < max_retries - 1:
                print(f"APIè°ƒç”¨å¤±è´¥ (attempt {attempt+1}/{max_retries}): {error_msg}")
                # æ£€æŸ¥æ˜¯å¦éœ€è¦é€€å‡º
                for i in range(retry_delay * (attempt + 1)):
                    if GRACEFUL_EXIT_REQUESTED:
                        return "è¯„ä¼°APIè°ƒç”¨å› è„šæœ¬é€€å‡ºè€Œä¸­æ­¢"
                    time.sleep(1)
            else:
                return f"è¯„ä¼°APIè°ƒç”¨å¤±è´¥: {error_msg}"
    
    return "è¯„ä¼°APIè°ƒç”¨å¤±è´¥: å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°"

def parse_scores(evaluation_text, task_type, subcategory=None):
    """è§£æè¯„ä¼°ç»“æœä¸­çš„åˆ†æ•°"""
    scores = {}
    
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    if task_type == "text_generation" and subcategory:
        print(f"Debug: æ­£åœ¨è§£æ{task_type}ä»»åŠ¡çš„è¯„åˆ†ï¼Œsubcategory: {subcategory}")
    
    # å®šä¹‰ç»´åº¦æ˜ å°„ï¼ˆä¸construct_promptä¸­ä¿æŒä¸€è‡´ï¼‰
    subcategory_to_answer_type = {
        "å¸¸è¯†çŸ¥è¯†": "äº‹å®ä¸è§£é‡Šå‹å›ç­”", "é˜…è¯»ç†è§£": "äº‹å®ä¸è§£é‡Šå‹å›ç­”", "ç¿»è¯‘": "ç”Ÿæˆå‹å›ç­”",
        "æ–‡æœ¬åˆ†ç±»": "äº‹å®ä¸è§£é‡Šå‹å›ç­”", "ä¿¡æ¯æŠ½å–": "äº‹å®ä¸è§£é‡Šå‹å›ç­”", "å­—è¯ç†è§£": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "æ–‡åŒ–ç†è§£": "äº‹å®ä¸è§£é‡Šå‹å›ç­”", "è§‚ç‚¹è¡¨è¾¾": "å»ºè®®å‹å›ç­”", "å¯»æ±‚å»ºè®®": "å»ºè®®å‹å›ç­”",
        "å®ç”¨æ–‡ä½“å†™ä½œ": "ç”Ÿæˆå‹å›ç­”", "åˆ›æ„æ–‡ä½“å†™ä½œ": "ç”Ÿæˆå‹å›ç­”", "ä¸“ä¸šæ–‡ä½“å†™ä½œ": "ç”Ÿæˆå‹å›ç­”",
        "å…¶ä»–å†™ä½œç±»": "ç”Ÿæˆå‹å›ç­”", "è¯æ˜": "é€»è¾‘æ¨ç†å‹å›ç­”", "æ¨ç†": "é€»è¾‘æ¨ç†å‹å›ç­”",
        "åˆç­‰æ•°å­¦": "é€»è¾‘æ¨ç†å‹å›ç­”", "é«˜ç­‰æ•°å­¦": "é€»è¾‘æ¨ç†å‹å›ç­”", "åº”ç”¨æ•°å­¦": "é€»è¾‘æ¨ç†å‹å›ç­”",
        "ç°å®ç”Ÿæ´»ç±»": "ç”Ÿæˆå‹å›ç­”", "æ¸¸æˆå¨±ä¹ç±»": "ç”Ÿæˆå‹å›ç­”", "åŠŸèƒ½ç±»": "ç”Ÿæˆå‹å›ç­”",
        "ç°å®åäººç±»": "ç”Ÿæˆå‹å›ç­”", "ï¼ˆè™šæ‹Ÿï¼‰æ‹çˆ±ç±»": "ç”Ÿæˆå‹å›ç­”", "ç‰©ç†": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "åŒ–å­¦": "äº‹å®ä¸è§£é‡Šå‹å›ç­”", "è®¡ç®—æœº": "äº‹å®ä¸è§£é‡Šå‹å›ç­”", "ç”Ÿç‰©åŒ»å­¦": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "ç»æµ": "äº‹å®ä¸è§£é‡Šå‹å›ç­”", "å¤©æ–‡": "äº‹å®ä¸è§£é‡Šå‹å›ç­”", "å†å²": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "éŸ³ä¹": "äº‹å®ä¸è§£é‡Šå‹å›ç­”", "æ³•å¾‹": "äº‹å®ä¸è§£é‡Šå‹å›ç­”", "ä½“è‚²": "äº‹å®ä¸è§£é‡Šå‹å›ç­”",
        "åœ°ç†": "äº‹å®ä¸è§£é‡Šå‹å›ç­”", "æ–‡å­¦": "äº‹å®ä¸è§£é‡Šå‹å›ç­”", "å…¶ä»–": "äº‹å®ä¸è§£é‡Šå‹å›ç­”"
    }
    
    answer_type_dimensions = {
        "äº‹å®ä¸è§£é‡Šå‹å›ç­”": ["äº‹å®æ­£ç¡®æ€§", "æ»¡è¶³ç”¨æˆ·éœ€æ±‚", "æ¸…æ™°åº¦", "å®Œå¤‡æ€§"],
        "é€»è¾‘æ¨ç†å‹å›ç­”": ["äº‹å®æ­£ç¡®æ€§", "æ»¡è¶³ç”¨æˆ·éœ€æ±‚", "é€»è¾‘è¿è´¯æ€§", "å®Œå¤‡æ€§"],
        "ç”Ÿæˆå‹å›ç­”": ["äº‹å®æ­£ç¡®æ€§", "æ»¡è¶³ç”¨æˆ·éœ€æ±‚", "é€»è¾‘è¿è´¯æ€§", "åˆ›é€ æ€§", "ä¸°å¯Œåº¦"],
        "å»ºè®®å‹å›ç­”": ["äº‹å®æ­£ç¡®æ€§", "æ»¡è¶³ç”¨æˆ·éœ€æ±‚", "å…¬å¹³ä¸å¯è´Ÿè´£ç¨‹åº¦", "åˆ›é€ æ€§"]
    }
    
    if task_type == "text_generation":
        # åŠ¨æ€æ„å»ºè¯„ä¼°ç»´åº¦ï¼ˆä¸construct_promptä¿æŒä¸€è‡´ï¼‰
        if subcategory and subcategory in subcategory_to_answer_type:
            answer_type = subcategory_to_answer_type[subcategory]
            dimensions = answer_type_dimensions[answer_type].copy()
            print(f"Debug: æ ¹æ®subcategory '{subcategory}' ç¡®å®šanswer_typeä¸º '{answer_type}'")
        else:
            dimensions = answer_type_dimensions["ç”Ÿæˆå‹å›ç­”"].copy()
            print(f"Debug: ä½¿ç”¨é»˜è®¤ç”Ÿæˆå‹å›ç­”ç»´åº¦ (subcategory: {subcategory})")
        
        # æ·»åŠ è¯­è¨€ä½¿ç”¨å‡†ç¡®æ€§ç»´åº¦
        dimensions.append("è¯­è¨€ä½¿ç”¨å‡†ç¡®æ€§")
        print(f"Debug: æœ€ç»ˆä½¿ç”¨çš„ç»´åº¦: {dimensions}")
        
        # åŠ¨æ€æ„å»ºæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œä½¿ç”¨æ ‡å‡†åŒ–çš„é”®å
        score_patterns = {}
        dimension_key_mapping = {
            "äº‹å®æ­£ç¡®æ€§": "factual_accuracy",
            "æ»¡è¶³ç”¨æˆ·éœ€æ±‚": "user_needs_satisfaction", 
            "æ¸…æ™°åº¦": "clarity",
            "å®Œå¤‡æ€§": "completeness",
            "é€»è¾‘è¿è´¯æ€§": "logical_coherence",
            "åˆ›é€ æ€§": "creativity",
            "ä¸°å¯Œåº¦": "richness",
            "å…¬å¹³ä¸å¯è´Ÿè´£ç¨‹åº¦": "fairness_responsibility",
            "è¯­è¨€ä½¿ç”¨å‡†ç¡®æ€§": "language_usage_accuracy"
        }
        
        for dim in dimensions:
            escaped_dim = dim.replace("(", r"\(").replace(")", r"\)").replace("ï¼ˆ", r"\ï¼ˆ").replace("ï¼‰", r"\ï¼‰")
            key = dimension_key_mapping.get(dim, dim.replace(" ", "_").lower())
            # âœ… ä¿®æ”¹ç‚¹: è®©æ­£åˆ™è¡¨è¾¾å¼å…¼å®¹æ–¹æ‹¬å·
            score_patterns[key] = f"{escaped_dim}[ï¼š:]\s*\[?([1-5](?:\.\d+)?)\]?"
        
        # æ·»åŠ åˆ†ææ€»ç»“å’Œæœ€ç»ˆåˆ†æ•°çš„æ¨¡å¼
        score_patterns["analysis_summary"] = r"åˆ†ææ€»ç»“[ï¼š:]\s*(.+?)(?=æœ€ç»ˆåˆ†æ•°[ï¼š:])"
        # âœ… ä¿®æ”¹ç‚¹: è®©æ­£åˆ™è¡¨è¾¾å¼å…¼å®¹æ–¹æ‹¬å·
        score_patterns["final_score"] = r"æœ€ç»ˆåˆ†æ•°[ï¼š:]\s*\[?([1-5](?:\.\d+)?)\]?"
        
        print(f"Debug: æ„å»ºçš„score_patterns keys: {list(score_patterns.keys())}")
        
    elif task_type == "traditional_culture":
        # âœ… ä¿®æ”¹ç‚¹: è®©æ‰€æœ‰åˆ†æ•°çš„æ­£åˆ™è¡¨è¾¾å¼éƒ½å…¼å®¹æ–¹æ‹¬å·
        score_patterns = {
            "knowledge_accuracy": r"çŸ¥è¯†å‡†ç¡®æ€§[ï¼š:]\s*\[?([1-5](?:\.\d+)?)\]?",
            "cultural_depth": r"æ–‡åŒ–ç†è§£æ·±åº¦[ï¼š:]\s*\[?([1-5](?:\.\d+)?)\]?",
            "expression": r"è¯­è¨€è¡¨è¾¾é€‚åˆ‡æ€§[ï¼š:]\s*\[?([1-5](?:\.\d+)?)\]?",
            "completeness": r"å†…å®¹å®Œæ•´æ€§[ï¼š:]\s*\[?([1-5](?:\.\d+)?)\]?",
            "insider_perspective": r"å†…éƒ¨è§†è§’çœŸå®æ€§[ï¼š:]\s*\[?([1-5](?:\.\d+)?)\]?",
            "language_usage_accuracy": r"è¯­è¨€ä½¿ç”¨å‡†ç¡®æ€§[ï¼š:]\s*\[?([1-5](?:\.\d+)?)\]?",
            "analysis_summary": r"åˆ†ææ€»ç»“[ï¼š:]\s*(.+?)(?=æœ€ç»ˆåˆ†æ•°[ï¼š:])",
            "final_score": r"æœ€ç»ˆåˆ†æ•°[ï¼š:]\s*\[?([1-5](?:\.\d+)?)\]?"
        }
    else:
        print(f"Debug: ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {task_type}")
        return {}

    # æ£€æŸ¥è¯„ä¼°æ–‡æœ¬æ˜¯å¦æœ‰æ•ˆ
    if evaluation_text is None or \
       evaluation_text.startswith("è¯„ä¼°APIè°ƒç”¨å¤±è´¥:") or \
       evaluation_text.startswith("è¯„ä¼°APIè°ƒç”¨å› è„šæœ¬é€€å‡ºè€Œä¸­æ­¢") or \
       evaluation_text.startswith("æ„å»ºpromptå¤±è´¥:"):
        print(f"Debug: è¯„ä¼°æ–‡æœ¬æ— æ•ˆ: {evaluation_text[:200] if evaluation_text else 'None'}")
        for key_pattern in score_patterns: 
            scores[key_pattern] = None
        return scores

    # è§£æè¯„åˆ†
    parsed_count = 0
    failed_patterns = []
    
    for key, pattern in score_patterns.items():
        match = re.search(pattern, evaluation_text, re.DOTALL)
        if match:
            if key == "analysis_summary":
                # å¤„ç†åˆ†ææ€»ç»“ï¼Œæ¸…ç†æ¢è¡Œå’Œå¤šä½™ç©ºæ ¼
                analysis = match.group(1).strip()
                analysis = re.sub(r'\s+', ' ', analysis)
                scores[key] = analysis
                parsed_count += 1
            else:
                try:
                    score_str = match.group(1)
                    if score_str is None: # å¤„ç†åŒ¹é…åˆ°ä½†æ•è·ç»„ä¸ºç©ºçš„æƒ…å†µï¼ˆä¸å¤ªå¯èƒ½ä½†ä¸ºäº†å¥å£®æ€§ï¼‰
                        raise ValueError("Captured group is None")
                    score = float(score_str)
                    # éªŒè¯åˆ†æ•°åœ¨1-5èŒƒå›´å†…
                    if 1 <= score <= 5:
                        scores[key] = score
                        parsed_count += 1
                    else:
                        print(f"Debug: åˆ†æ•°è¶…å‡ºèŒƒå›´ {key}: {score}")
                        scores[key] = None
                        failed_patterns.append(f"{key}(è¶…å‡ºèŒƒå›´:{score})")
                except (ValueError, TypeError):
                    print(f"Debug: æ— æ³•è§£æåˆ†æ•° {key}: {match.group(1)}")
                    scores[key] = None
                    failed_patterns.append(f"{key}(è§£æé”™è¯¯:{match.group(1)})")
        else:
            scores[key] = None
            failed_patterns.append(f"{key}(æœªæ‰¾åˆ°)")
    
    print(f"Debug: æˆåŠŸè§£æ {parsed_count}/{len(score_patterns)} ä¸ªè¯„åˆ†")
    
    # å¦‚æœè§£æå¤±è´¥ï¼Œè¾“å‡ºè¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
    if parsed_count < len(score_patterns) and parsed_count > 0:
        print("="*80)  
        print("âš ï¸  DEBUG: éƒ¨åˆ†è§£æå¤±è´¥ï¼Œä»¥ä¸‹æ˜¯è¯¦ç»†ä¿¡æ¯:")
        print(f"ğŸ“‹ Task: {task_type}, Subcategory: {subcategory}")
        print(f"âœ… æˆåŠŸè§£æ: {parsed_count}/{len(score_patterns)}")
        print(f"âŒ å¤±è´¥çš„æ¨¡å¼: {failed_patterns}")
        print("\nğŸ“ è¯„ä¼°æ¨¡å‹çš„å®Œæ•´åŸå§‹å›å¤:")
        print("-"*50)
        print(evaluation_text)
        print("-"*50)
        print("="*80)
    elif parsed_count == 0:
        print("="*80)
        print("ğŸ” DEBUG: å®Œå…¨æ— æ³•è§£æè¯„åˆ†ï¼Œä»¥ä¸‹æ˜¯è¯¦ç»†ä¿¡æ¯:")
        print(f"ğŸ“‹ Task: {task_type}, Subcategory: {subcategory}")  
        print(f"ğŸ¯ æœŸæœ›çš„è¯„åˆ†æ¨¡å¼: {list(score_patterns.keys())}")
        print(f"âŒ å¤±è´¥çš„æ¨¡å¼: {failed_patterns}")
        print("\nğŸ“ è¯„ä¼°æ¨¡å‹çš„å®Œæ•´åŸå§‹å›å¤:")
        print("-"*50)
        print(evaluation_text)
        print("-"*50)
        print("\nğŸ” æœŸæœ›çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼:")
        for key, pattern in score_patterns.items():
            print(f"  {key}: {pattern}")
        print("="*80)

    # å¦‚æœLLMæ²¡æœ‰æä¾›æœ€ç»ˆåˆ†æ•°æˆ–æœªèƒ½è§£æï¼Œåˆ™è®¡ç®—å¹³å‡å€¼
    if scores.get("final_score") is None:
        valid_sub_scores = [v for k, v in scores.items() if k not in ["final_score", "analysis_summary"] and v is not None]
        if valid_sub_scores: 
            scores["final_score"] = round(sum(valid_sub_scores) / len(valid_sub_scores), 2)
            print(f"Debug: è®¡ç®—å¹³å‡æœ€ç»ˆåˆ†æ•°: {scores['final_score']}")
        else:
            print("Debug: æ²¡æœ‰æœ‰æ•ˆçš„å­åˆ†æ•°ï¼Œæ— æ³•è®¡ç®—æœ€ç»ˆåˆ†æ•°")
    
    return scores

def is_evaluation_successful(evaluation_result_dict):
    """æ£€æŸ¥è¯„ä¼°æ˜¯å¦æˆåŠŸ"""
    if not isinstance(evaluation_result_dict, dict): 
        return False
    
    eval_text = evaluation_result_dict.get("evaluation", "")
    if eval_text is None or \
       eval_text.startswith("è¯„ä¼°APIè°ƒç”¨å¤±è´¥:") or \
       eval_text.startswith("è¯„ä¼°APIè°ƒç”¨å› è„šæœ¬é€€å‡ºè€Œä¸­æ­¢"):
        return False
    
    return evaluation_result_dict.get("final_score") is not None

def validate_evaluation_result(result_dict, task_type, subcategory=None):
    """éªŒè¯è¯„ä¼°ç»“æœçš„å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§"""
    if not isinstance(result_dict, dict):
        return False, "ç»“æœä¸æ˜¯å­—å…¸æ ¼å¼"
    
    # æ£€æŸ¥å¿…éœ€çš„å­—æ®µ
    required_fields = ["id", "model", "task_type", "evaluation", "final_score"]
    missing_fields = [field for field in required_fields if field not in result_dict]
    if missing_fields:
        return False, f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}"
    
    # æ£€æŸ¥final_scoreæ˜¯å¦æœ‰æ•ˆ
    final_score = result_dict.get("final_score")
    if final_score is None or not isinstance(final_score, (int, float)) or not (1 <= final_score <= 5):
        return False, f"final_scoreæ— æ•ˆ: {final_score}"
    
    # æ ¹æ®ä»»åŠ¡ç±»å‹æ£€æŸ¥ç‰¹å®šå­—æ®µ
    if task_type == "text_generation":
        expected_scores = ["factual_accuracy", "user_needs_satisfaction", "language_usage_accuracy"]
    elif task_type == "traditional_culture":
        expected_scores = ["knowledge_accuracy", "cultural_depth", "expression", "completeness", "insider_perspective", "language_usage_accuracy"]
    else:
        return False, f"æœªçŸ¥ä»»åŠ¡ç±»å‹: {task_type}"
    
    # æ£€æŸ¥è‡³å°‘æœ‰ä¸€äº›ç»´åº¦åˆ†æ•°
    score_fields = [field for field in expected_scores if field in result_dict and result_dict[field] is not None]
    if len(score_fields) == 0:
        return False, "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ç»´åº¦åˆ†æ•°"
    
    return True, "éªŒè¯é€šè¿‡"

# --- Command Line Argument Parsing ---
def parse_args():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨LLMè¯„ä¼°æ¨¡å‹è¾“å‡ºè´¨é‡")
    parser.add_argument("--test_data_path", type=str, required=True, help="æµ‹è¯•æ•°æ®é›†åŸºç¡€è·¯å¾„")
    parser.add_argument("--models_predictions_path", type=str, required=True, help="æ¨¡å‹é¢„æµ‹ç»“æœåŸºç¡€è·¯å¾„")
    parser.add_argument("--output_path", type=str, required=True, help="è¯„ä¼°ç»“æœè¾“å‡ºè·¯å¾„")
    parser.add_argument("--api_key", type=str, required=True, help="OpenAI APIå¯†é’¥")
    parser.add_argument("--api_base", type=str, default=None, help="APIåŸºç¡€URL")
    parser.add_argument("--model", type=str, default="gpt-4o", help="ç”¨äºè¯„ä¼°çš„LLMæ¨¡å‹åç§°")
    parser.add_argument("--max_workers", type=int, default=5, help="å¹¶è¡Œå¤„ç†çº¿ç¨‹æ•°")
    parser.add_argument("--sample_size", type=int, default=None, help="æ¯ä¸ªä»»åŠ¡è¯„ä¼°çš„æ ·æœ¬æ•° (None for all)")
    parser.add_argument("--models_to_evaluate", nargs='+', default=None, help="è¦è¯„ä¼°çš„æ¨¡å‹åˆ—è¡¨ (None for all in models_predictions_path)")
    parser.add_argument("--resume", action="store_true", help="ä»ä¸Šæ¬¡ä¸­æ–­çš„åœ°æ–¹ç»§ç»­æ‰§è¡Œ")
    parser.add_argument("--task", type=str, help="ç‰¹å®šä»»åŠ¡ç±»å‹ (traditional_culture, text_generation)")
    parser.add_argument("--language", type=str, help="ç‰¹å®šè¯­è¨€ (e.g., bo, mn, ug)")
    parser.add_argument("--checkpoint_interval_items", type=int, default=10, help="æ¯å¤„ç†Nä¸ªé¡¹ç›®åï¼Œè‹¥æœ‰æˆåŠŸé¡¹åˆ™ä¿å­˜æ£€æŸ¥ç‚¹")
    parser.add_argument("--checkpoint_interval_time", type=int, default=300, help="æ¯Nç§’åï¼Œè‹¥æœ‰æˆåŠŸé¡¹åˆ™ä¿å­˜æ£€æŸ¥ç‚¹")
    return parser.parse_args()

# --- Main Processing Function ---
def process_task(args, client, eval_model_name, task_type, language_param_for_file):
    global GRACEFUL_EXIT_REQUESTED

    if GRACEFUL_EXIT_REQUESTED: 
        return

    # å¯¹äºéç¿»è¯‘ä»»åŠ¡ï¼Œlanguage_param_for_fileå°±æ˜¯ç›®æ ‡è¯­è¨€
    language_for_data = language_param_for_file

    print(f"\n--- å¼€å§‹å¤„ç†: æ¨¡å‹={eval_model_name}, ä»»åŠ¡={task_type}, è¯­è¨€å‚æ•°={language_param_for_file} ---")

    # â­ ä¿®æ”¹è¿™é‡Œï¼šä½¿ç”¨æ–°çš„ä»»åŠ¡åç§°ä½œä¸ºè¾“å‡ºæ–‡ä»¶å¤¹åç§°
    new_task_name = REVERSE_TASK_MAPPING.get(task_type, task_type)
    task_specific_output_dir = os.path.join(args.output_path, eval_model_name, new_task_name)
    os.makedirs(task_specific_output_dir, exist_ok=True)
    
    base_filename = f"{language_param_for_file}"
    output_file = os.path.join(task_specific_output_dir, f"{base_filename}_evaluation.json")
    checkpoint_file = os.path.join(task_specific_output_dir, f"{base_filename}_checkpoint.json")
    error_log_file = os.path.join(task_specific_output_dir, f"{base_filename}_errors.log")
    error_id_file = os.path.join(task_specific_output_dir, f"{base_filename}_error_ids.json")

    print(f"è¾“å‡ºç›®å½•: {task_specific_output_dir}")  # æ·»åŠ æ—¥å¿—ç¡®è®¤è¾“å‡ºç›®å½•

    # State variables
    successful_results_list = []
    processed_successful_ids_set = set()

    # Resume logic
    if args.resume:
        print(f"å°è¯•ä»æ–­ç‚¹æ¢å¤ (ä¸»è¾“å‡ºæ–‡ä»¶ä¼˜å…ˆ): {output_file}")
        loaded_from_output = False
        
        if os.path.exists(output_file):
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    all_previous_results = json.load(f)
                
                if isinstance(all_previous_results, list):
                    for res in all_previous_results:
                        if isinstance(res, dict) and 'id' in res and is_evaluation_successful(res):
                            if res['id'] not in processed_successful_ids_set:
                                successful_results_list.append(res)
                                processed_successful_ids_set.add(res['id'])
                    
                    if successful_results_list:
                        loaded_from_output = True
                        print(f"ä»è¾“å‡ºæ–‡ä»¶ {output_file} åŠ è½½äº† {len(successful_results_list)} æ¡æˆåŠŸçš„è¯„ä¼°ç»“æœã€‚")
                else:
                    print(f"è­¦å‘Š: è¾“å‡ºæ–‡ä»¶ {output_file} æ ¼å¼ä¸æ­£ç¡®ã€‚å°è¯•å¤‡ä»½...")
                    backup_file = output_file + f".invalid.{int(time.time())}"
                    os.rename(output_file, backup_file)
            except Exception as e_load_output:
                print(f"è¯»å–æˆ–å¤„ç†è¾“å‡ºæ–‡ä»¶ {output_file} æ—¶å‡ºé”™: {str(e_load_output)}. å°è¯•å¤‡ä»½...")
                try: 
                    backup_file = output_file + f".corrupt.{int(time.time())}"
                    os.rename(output_file, backup_file)
                except OSError: 
                    pass
        
        if not loaded_from_output and os.path.exists(checkpoint_file):
            print(f"å°è¯•ä»æ£€æŸ¥ç‚¹æ–‡ä»¶æ¢å¤: {checkpoint_file}")
            try:
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    checkpoint_data = json.load(f)
                
                temp_results = checkpoint_data.get("successful_evaluation_results", [])
                temp_ids = set(checkpoint_data.get("processed_successful_ids", []))
                
                for res in temp_results:
                     if isinstance(res, dict) and 'id' in res and res['id'] in temp_ids and is_evaluation_successful(res):
                         if res['id'] not in processed_successful_ids_set:
                            successful_results_list.append(res)
                            processed_successful_ids_set.add(res['id'])
                            
                print(f"ä»æ£€æŸ¥ç‚¹ {checkpoint_file} åŠ è½½åï¼Œæ€»è®¡ {len(processed_successful_ids_set)} ä¸ªå·²æˆåŠŸå¤„ç†çš„IDã€‚")
            except Exception as e_load_cp:
                print(f"åŠ è½½æ£€æŸ¥ç‚¹ {checkpoint_file} æ—¶å‡ºé”™: {str(e_load_cp)}. å°è¯•å¤‡ä»½å¹¶å¿½ç•¥æ­¤æ£€æŸ¥ç‚¹ã€‚")
                try: 
                    backup_file = checkpoint_file + f".corrupt.{int(time.time())}"
                    os.rename(checkpoint_file, backup_file)
                except OSError: 
                    pass

    # Load data
    all_test_data_map = load_test_data(task_type, language_for_data, args.test_data_path)
    if not all_test_data_map:
        log_error(error_log_file, error_id_file, "æµ‹è¯•æ•°æ®åŠ è½½å¤±è´¥", None, details=f"{task_type}/{language_for_data}")
        return

    all_model_predictions = load_model_predictions(eval_model_name, task_type, language_param_for_file, args.models_predictions_path)
    if not all_model_predictions:
        log_error(error_log_file, error_id_file, "æ¨¡å‹é¢„æµ‹åŠ è½½å¤±è´¥", None, details=f"{eval_model_name}/{task_type}/{language_param_for_file}")
        return

    items_to_process = [
        item for item in all_model_predictions 
        if isinstance(item, dict) and item.get('id') and item.get('id') not in processed_successful_ids_set
    ]

    if not items_to_process:
        print(f"æ‰€æœ‰ {len(all_model_predictions)} ä¸ªæ ·æœ¬å·²æˆåŠŸè¯„ä¼°æˆ–æ— å¯å¤„ç†æ ·æœ¬ã€‚")
        if successful_results_list:
            save_json_safely(successful_results_list, output_file)
        if os.path.exists(checkpoint_file):
            try: 
                os.remove(checkpoint_file)
                print(f"ä»»åŠ¡å®Œæˆï¼Œæ£€æŸ¥ç‚¹ {checkpoint_file} å·²åˆ é™¤ã€‚")
            except OSError as e_rm: 
                print(f"æ— æ³•åˆ é™¤æ£€æŸ¥ç‚¹ {checkpoint_file}: {e_rm}")
        return

    print(f"æ€»å…± {len(all_model_predictions)} ä¸ªé¢„æµ‹ï¼Œå…¶ä¸­ {len(items_to_process)} ä¸ªå¾…å¤„ç† (ä¹‹å‰æˆåŠŸ {len(processed_successful_ids_set)} ä¸ª)ã€‚")

    if args.sample_size and args.sample_size > 0 and args.sample_size < len(items_to_process):
        random.seed(42)
        items_to_process = random.sample(items_to_process, args.sample_size)
        print(f"éšæœºæŠ½æ · {len(items_to_process)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")
    
    current_run_newly_successful_count = 0
    current_run_failed_item_count = 0
    last_checkpoint_time = time.time()
    processed_item_counter_current_run = 0

    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=args.max_workers) as executor:
            futures_map = {}
            
            for item_pred in items_to_process:
                if GRACEFUL_EXIT_REQUESTED: 
                    break
                
                item_id = item_pred.get('id')
                if not item_id: 
                    log_error(error_log_file, error_id_file, "é¢„æµ‹æ•°æ®ç¼ºå°‘ID", None, details=str(item_pred))
                    continue
                
                original_data_item = all_test_data_map.get(item_id)
                if not original_data_item:
                    log_error(error_log_file, error_id_file, "æ‰¾ä¸åˆ°åŸå§‹æµ‹è¯•æ•°æ®", None, [item_id], f"Task: {task_type}, Lang: {language_for_data}")
                    continue
                
                # å¯¹äºæ‰€æœ‰éç¿»è¯‘ä»»åŠ¡ï¼Œéƒ½ä½¿ç”¨questionå’Œanswerå­—æ®µ
                question = original_data_item.get('question', '')
                reference = original_data_item.get('answer', '')
                
                prediction = item_pred.get('answer', item_pred.get('pred', ''))
                
                # æå–subcategory
                subcategory = None
                if task_type == "text_generation" and isinstance(item_pred, dict):
                    subcategory = item_pred.get('subcategory')
                    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                    if subcategory:
                        print(f"Debug: æ ·æœ¬ {item_id} æå–åˆ°subcategory: {subcategory}")
                    else:
                        print(f"Debug: æ ·æœ¬ {item_id} æ²¡æœ‰subcategoryä¿¡æ¯ (keys: {list(item_pred.keys())})")

                future = executor.submit(evaluate_sample, client, args.model, task_type, language_for_data, question, reference, prediction, subcategory)
                futures_map[future] = (item_id, question, reference, prediction, subcategory)

            # ä½¿ç”¨ as_completed ä½†æ·»åŠ è¶…æ—¶
            completed_futures = []
            try:
                for future in tqdm(concurrent.futures.as_completed(futures_map, timeout=60), total=len(futures_map), desc=f"è¯„ä¼° {eval_model_name}/{new_task_name}/{language_param_for_file}"):
                    completed_futures.append(future)
                    if GRACEFUL_EXIT_REQUESTED:
                        print("\næ£€æµ‹åˆ°é€€å‡ºè¯·æ±‚ï¼Œåœ¨tqdmå¾ªç¯ä¸­æå‰ç»ˆæ­¢ã€‚")
                        break
            except concurrent.futures.TimeoutError:
                print("\néƒ¨åˆ†ä»»åŠ¡è¶…æ—¶ï¼Œå°†å¤„ç†å·²å®Œæˆçš„ä»»åŠ¡...")
                completed_futures = [f for f in futures_map.keys() if f.done()]
            
            # å–æ¶ˆæœªå®Œæˆçš„ä»»åŠ¡
            if GRACEFUL_EXIT_REQUESTED:
                for f_cancel in futures_map.keys():
                    if not f_cancel.done(): 
                        f_cancel.cancel()

            # å¤„ç†å·²å®Œæˆçš„ä»»åŠ¡
            for future in completed_futures:
                if future not in futures_map:
                    continue
                    
                item_id, question, reference, prediction, subcategory = futures_map[future]
                processed_item_counter_current_run += 1
                
                try:
                    if future.cancelled():
                        log_error(error_log_file, error_id_file, "ä»»åŠ¡è¢«å–æ¶ˆ", None, [item_id], "å¯èƒ½ç”±äºè„šæœ¬é€€å‡ºä¿¡å·")
                        current_run_failed_item_count += 1
                        continue

                    evaluation_text = future.result(timeout=5)  # 5ç§’è·å–ç»“æœè¶…æ—¶
                    scores = parse_scores(evaluation_text, task_type, subcategory)
                    result_dict = {
                        "id": item_id, 
                        "model": eval_model_name, 
                        "task_type": task_type,
                        "subcategory": subcategory,
                        "language_param": language_param_for_file, 
                        "language_evaluated": language_for_data,
                        "question": question, 
                        "reference": reference, 
                        "prediction": prediction,
                        "evaluation": evaluation_text, 
                        **scores
                    }

                    # ä½¿ç”¨æ›´ä¸¥æ ¼çš„éªŒè¯
                    is_valid, validation_msg = validate_evaluation_result(result_dict, task_type, subcategory)
                    if is_valid and is_evaluation_successful(result_dict):
                        if item_id not in processed_successful_ids_set:
                           successful_results_list.append(result_dict)
                           processed_successful_ids_set.add(item_id)
                        current_run_newly_successful_count += 1
                    else:
                        current_run_failed_item_count += 1
                        error_detail = f"éªŒè¯å¤±è´¥: {validation_msg}" if not is_valid else "è¯„ä¼°ä¸æˆåŠŸ"
                        log_error(error_log_file, error_id_file, error_detail, None, [item_id], 
                                evaluation_text[:500] if evaluation_text else "æ— è¯„ä¼°æ–‡æœ¬")

                except (concurrent.futures.TimeoutError, Exception) as e_item_processing:
                    current_run_failed_item_count += 1
                    log_error(error_log_file, error_id_file, "å¤„ç†æ ·æœ¬æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯", e_item_processing, [item_id])
                    continue
                
                # Checkpoint saving logic
                time_now = time.time()
                if (processed_item_counter_current_run > 0 and processed_item_counter_current_run % args.checkpoint_interval_items == 0) or \
                   (time_now - last_checkpoint_time > args.checkpoint_interval_time):
                    if successful_results_list:
                        print(f"\nè§¦å‘æ£€æŸ¥ç‚¹ä¿å­˜ (æœ¬è½®å·²å¤„ç† {processed_item_counter_current_run} é¡¹, æ—¶é—´è‡ªä¸Šæ¬¡: {time_now - last_checkpoint_time:.0f}s)...")
                        save_json_safely({
                            "processed_successful_ids": list(processed_successful_ids_set),
                            "successful_evaluation_results": successful_results_list,
                            "timestamp": time_now
                        }, checkpoint_file)
                        last_checkpoint_time = time_now

    except KeyboardInterrupt:
        print("\næ•è·åˆ° KeyboardInterruptï¼Œå‡†å¤‡é€€å‡º...")
        GRACEFUL_EXIT_REQUESTED = True
    except Exception as e_main_loop:
        print(f"\nä¸»å¤„ç†å¾ªç¯ä¸­å‘ç”Ÿæœªæ•è·å¼‚å¸¸: {str(e_main_loop)}")
        log_error(error_log_file, error_id_file, "ä¸»å¤„ç†å¾ªç¯é”™è¯¯", e_main_loop)
        GRACEFUL_EXIT_REQUESTED = True
    finally:
        print("\nè¿›å…¥æœ€ç»ˆä¿å­˜é˜¶æ®µ...")
        
        # Always save the current state
        if successful_results_list or processed_successful_ids_set:
            print(f"ä¿å­˜ {len(successful_results_list)} æ¡æˆåŠŸè¯„ä¼°ç»“æœåˆ°ä¸»è¾“å‡ºæ–‡ä»¶...")
            save_json_safely(successful_results_list, output_file)
            
            print(f"ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹ (åŒ…å« {len(processed_successful_ids_set)} ä¸ªæˆåŠŸID)...")
            save_json_safely({
                "processed_successful_ids": list(processed_successful_ids_set),
                "successful_evaluation_results": successful_results_list,
                "timestamp": time.time()
            }, checkpoint_file)
        else:
            print("æ²¡æœ‰æˆåŠŸçš„è¯„ä¼°ç»“æœå¯ä¿å­˜ã€‚")

        # Determine if the task is fully complete
        is_fully_complete = True
        if all_model_predictions and len(processed_successful_ids_set) < len(all_model_predictions):
            is_fully_complete = False

        if is_fully_complete and not GRACEFUL_EXIT_REQUESTED and current_run_failed_item_count == 0:
            if os.path.exists(checkpoint_file):
                try:
                    os.remove(checkpoint_file)
                    print(f"ä»»åŠ¡å·²å®Œå…¨æˆåŠŸï¼Œæ£€æŸ¥ç‚¹æ–‡ä»¶ {checkpoint_file} å·²åˆ é™¤ã€‚")
                except OSError as e_rm_final:
                    print(f"è­¦å‘Š: æ— æ³•åˆ é™¤å·²å®Œæˆä»»åŠ¡çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ {checkpoint_file}: {e_rm_final}")
        elif GRACEFUL_EXIT_REQUESTED:
            print(f"ç”±äºè„šæœ¬ä¸­æ–­ï¼Œæ£€æŸ¥ç‚¹ {checkpoint_file} å·²ä¿ç•™ã€‚")
        else:
            print(f"ä»»åŠ¡æœªå®Œå…¨æˆåŠŸæˆ–æœ‰å¤±è´¥é¡¹ï¼Œæ£€æŸ¥ç‚¹ {checkpoint_file} å·²ä¿ç•™ã€‚")

        print(f"--- å¤„ç†ç»“æŸ: æ¨¡å‹={eval_model_name}, ä»»åŠ¡={new_task_name}, è¯­è¨€å‚æ•°={language_param_for_file} ---")
        print(f"æœ¬è½®æ–°æˆåŠŸè¯„ä¼°æ•°: {current_run_newly_successful_count}, æœ¬è½®å¤„ç†å¤±è´¥/è·³è¿‡é¡¹æ•°: {current_run_failed_item_count}")
        print(f"æ€»è®¡å·²æˆåŠŸè¯„ä¼°IDæ•° (åŒ…æ‹¬å†å²): {len(processed_successful_ids_set)}")
        if os.path.exists(error_id_file) or os.path.exists(error_log_file):
             print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯å’ŒIDå·²è®°å½•åˆ°ä¸è¾“å‡ºæ–‡ä»¶åŒç›®å½•çš„ _errors.log å’Œ _error_ids.json æ–‡ä»¶ã€‚")

def main():
    # 1. ç›´æ¥è°ƒç”¨ parse_args() æ¥è·å–æ‰€æœ‰æ¥è‡ªå‘½ä»¤è¡Œçš„å‚æ•°
    args = parse_args()
    
    os.makedirs(args.output_path, exist_ok=True)

    try:
        client = get_client(args.api_key, args.api_base)
    except Exception as e:
        print(f"åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥API Keyæ˜¯å¦æ­£ç¡®è®¾ç½®ã€‚")
        return

    models_to_run = args.models_to_evaluate
    if not models_to_run:
        try:
            if not os.path.exists(args.models_predictions_path):
                 print(f"é”™è¯¯: æ¨¡å‹é¢„æµ‹è·¯å¾„ '{args.models_predictions_path}' ä¸å­˜åœ¨ã€‚")
                 return
            models_to_run = [d for d in os.listdir(args.models_predictions_path) if os.path.isdir(os.path.join(args.models_predictions_path, d))]
            if not models_to_run:
                print(f"é”™è¯¯: åœ¨ {args.models_predictions_path} ä¸­æœªæ‰¾åˆ°æ¨¡å‹ç›®å½•ï¼Œä¸”æœªé€šè¿‡ --models_to_evaluate æŒ‡å®šæ¨¡å‹ã€‚")
                return
            print(f"å°†è¯„ä¼°åœ¨ {args.models_predictions_path} ä¸­æ‰¾åˆ°çš„æ‰€æœ‰æ¨¡å‹: {models_to_run}")
        except FileNotFoundError:
            print(f"é”™è¯¯: æ¨¡å‹åŸºç¡€è·¯å¾„ {args.models_predictions_path} æœªæ‰¾åˆ°ã€‚è¯·é€šè¿‡ --models_to_evaluate æŒ‡å®šæ¨¡å‹ã€‚")
            return
    
    if isinstance(models_to_run, str): 
        models_to_run = [models_to_run]

    if args.task and args.language:
        # å¤„ç†æŒ‡å®šçš„å•ä¸ªä»»åŠ¡
        for model_name_to_eval in models_to_run:
            if GRACEFUL_EXIT_REQUESTED: 
                break
            process_task(args, client, model_name_to_eval, args.task, args.language)
    else:
        # å¤„ç†æ‰€æœ‰ç”Ÿæˆå¼ä»»åŠ¡
        ethnic_languages = ["bo", "mn", "ug"]
        all_task_keys = list(TASK_TYPES.keys())

        for model_name_to_eval in models_to_run:
            if GRACEFUL_EXIT_REQUESTED: 
                break
            print(f"\n===== å¼€å§‹å¤„ç†æ¨¡å‹: {model_name_to_eval} =====")
            
            for task_key in all_task_keys:
                if GRACEFUL_EXIT_REQUESTED: 
                    break
                
                # å…¶ä»–ä»»åŠ¡æŒ‰è¯­è¨€å¤„ç†
                for lang_code in ethnic_languages:
                    if GRACEFUL_EXIT_REQUESTED: 
                        break
                    process_task(args, client, model_name_to_eval, task_key, lang_code)
    
    if GRACEFUL_EXIT_REQUESTED:
        print("\nè¯„ä¼°è¿‡ç¨‹å› å¤–éƒ¨ä¿¡å·è¢«ä¸­æ–­ã€‚éƒ¨åˆ†ä»»åŠ¡å¯èƒ½æœªå®Œæˆã€‚")
    else:
        print("\næ‰€æœ‰è¯„ä¼°ä»»åŠ¡å¤„ç†å®Œæ¯•ã€‚")

# 2. ç¡®ä¿è„šæœ¬è¢«æ‰§è¡Œæ—¶ï¼Œç›´æ¥è°ƒç”¨ main å‡½æ•°
if __name__ == "__main__":
    main()